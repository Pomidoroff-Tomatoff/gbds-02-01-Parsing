Методы сбора и обработки данных из сети Интернет
-- практический курс 

Преподаватель: 
    Эрик Муллагалиев
        https://github.com/erik-mv/
Наставник: 
    Андрей Опанасенко, студент
        https://gb.ru/users/5740504
Ревьювер:
    Вадим Мазейко, преподаватель
        https://gb.ru/users/teachers/3293490

Канал (сами организовали): https://t.me/+subDDrTGemFiZGMy

УСТАНОВКА ПО 
    ConEmu v.161206
        -- более удобный аналог CLI для Windows.

    Postman 7.5.0
        -- таскалка (из инета?)

    PyCharm Education 2019.1.1
        -- среда разработки Python

    Python v.3.7.2 + Anaconda
        -- интерпретатор Python

        -- Виртуальные окружения
            > Переход в виртуальное пространство Питона
                    activate.bat          -- скорее, устаревшая команда самого Питона, так как отправляет к conda.bat
                >> вызывает conda.bat с параметром activate:
                    conda activate 
                        (base) X:\...>    -- без параметра -- базовое окружение
                >> прямая команда:
                    conda activate scrapy
                        (scrapy) X:\...>
            > Выход:
                    conda deactivate
                или устаревная команда:
                    deactivte

    cURL
        -- кросплатформенная утилита для взаимодействия с множеством серверов используя различные протоколы с синтаксисом URL 

    Python requests
        -- Библиотека "requests"
            > обычная установка
                pip install requests 
            > установка с использованием виртуальной среды
                ???? что-то здесь не так ???
                pipenv install requests 

    XPath 
        -- Расширение "ChroPath" для браузера Хром  (требуется установить)
        -- Модуль для Python: lxml 
            > обычная установка
                pip install lxml
    
    Beautiful Soap:
        -- парсим стринички 
        -- библиотека "BeautifulSoap" в модуле "bs4" Python 
            > обычная установка
                pip install bs4
        -- загрузка в Питоне 
            from bs4 import BeautifulSoup as bs

    MongoDB: 
        -- port: 27017
        -- Версия для Windows 7: 4.2.23
            > Всё автоматически, но!
            > Указать на папку без пробелов (c:\programs\mongodb) -- это лучше!
            > Среду Compass устанавливать отдельно, а то установка зависнет!
            > В переменную окружения Path (можно в системную) добавить маршрут к папке "C:\Programs\MongoDB\bin\;"

        -- Версия для Windows 7: 3.2.22 (или 3.4), все остальные зависают в конце установки...
            https://docs.mongodb.com/manual/administration/install-community/
            > Настройка сервера (сервиса): выполняется вручную в этой версии (3.x)
                >> папка по умолчанию для базы данных: 
                    c:\data\db
                >> если папка для данных своя, то её необходимо создать вручную и указать параметром --dbpath
                    --dbpath C:\Programs\MongoDB\Server\data\db\
                >> файл логов необходимо создать вручную и его расположение указать параметром --logpath
                    --logpath C:\Programs\MongoDB\log\mongo.log
                >> команда инициализации сервиса для сервера mongodb:
                    mongod --dbpath C:\Programs\MongoDB\Server\data\db\ --logpath C:\Programs\MongoDB\log\mongo.log  --install
                   Примечание: если всё в порядке, то никакого стардартого вывода не будет вообще...
                >> далее запускаем сервис
                    net start mongodb
                    (или поставить автозагрузку при старе системы в консоли)
            > При конфигурирвоание сервиса при помощи файла конфигурации:
                https://stackoverflow.com/questions/23726684/mongodb-on-a-windows-7-machine-no-connection-could-be-made
                Файл конфигурации:
                    dbpath=C:\Programs\MongoDB\data\
                    logpath=C:\Programs\MongoDB\log\mongo.log
                    diaglog=3
                Прим.: что такое "diaglog=3" -- не знаю...
                Старт с файлом конфигурации:
                    mongod.exe --config c:\mongodb\bin\mongo.config
                А это вроде как запуск сервера...
                    mongod.exe --storageEngine=mmapv1 --config=C:\Programs\MongoDB\mongo.config 
            > В переменную окружения Path (можно в системную) добавить маршрут к папке "C:\Programs\MongoDB\bin\;" 

            > Запуск сервера как сервиса (Linux)
                brew services start mongodb-community@5.0

        -- Python: модуль в Python:
            pip install pymongo
        -- модуль в Anaconda -- там он уже есть!
            > текущее окружение
                conda install pymongo
            > в создаваемое новое окружение с именем "MongoDB":
                conda create --name MongoDB pymongo 
            > в имеющееся окружение с именем "MongoDB" из канала conda-forge:
                conda install --name MongoDB -c conda-forge pymongo

    Scrapy 
        https://docs.scrapy.org/en/latest/intro/install.html
        -- входит в PyPI (Python Package Index, так же известен как pip)
            pip install scrapy 
        -- виртуальное окружение 
            - активировать виртуальное окружение (перейти в него)
                ...\путь-к-вирт-окружению\activate.bat
            - проверить, откуда берётся интерпретатор питона
                where python
            - обычная команда установки 
                pip ...
        -- conda так же знает о scrapy
            > установка в текущее окружение (стандартная установка)
                conda install -c conda-forge scrapy
            > установка в окружение с указанием его имени
              это окружение уже существует
                conda install --name ИмяВиртОружения -c conda-forge scrapy 

    Splash -- plugin Scrapy-Splash (плагин Скрэйпи)
        https://anaconda.org/search?q=splash
        https://splash.readthedocs.io/
        https://github.com/scrapy-plugins/scrapy-splash

        Установка и настройка:
            https://splash.readthedocs.io/en/latest/install.html

        -- Python Package Index
            pip scrapy 3.5 == v.0.9.0
            pip install scrapy-splash
        -- Standard Python
            v.0.7.2
            https://anaconda.org/yequaltoaxaddb/scrapy-splash
                pip install -i https://pypi.anaconda.org/yequaltoaxaddb/simple scrapy-splash
        -- Docker
            Установка и использование самого docker:
                EN:         https://www.zyte.com/blog/handling-javascript-in-scrapy-with-splash/
                info EN:    https://docs.docker.com/get-started/
                App:        https://www.docker.com/get-started/
                            начиная с Window 10

                sudo docker pull scrapinhub/splash

            Запуск и настройка докера с плагином splash
                $ docker run -p 8050:8050 scrapinghub/splash
                или 
                $ sudo docker run -it -p 192.168.0.103:8050:8050 --rm scrapinghub/splash
                    --rm  удалить образ после завершения

        -- conda: Anaconda, Orchardmile
            v.0.6.1
            https://anaconda.org/orchardmile/scrapy-splash
            Linux, OSX:
                conda install -c orchardmile scrapy-splash
                conda install -c "orchardmile/label/dev" scrapy-splash
            v.0.7.2 (установил в свою анаконду)
            https://anaconda.org/Bjrn/scrapy-splash
            Windows, Linux, OSX:
                conda install -c bjrn scrapy-splash

    Splash and Jupiter
        https://splash.readthedocs.io/en/stable/kernel.html#installation
        Установка: ???
            $ docker pull scrapinghub/splash-jupyter
            -- работа со Splash в блокнотах Jupiter 
            -- с этим очень интересно разобраться...

    Selenium, установка 
        1. Selenium
            > стандартная (с обновлением  -U, --upgrade):
                pip install -U selenium 
            > в Анаконду (командная строка)
                >> текущее окружение
                    conda install selenium
                >> в создаваемое новое окружение с именем "ИмяВиртОкружения":
                    conda create --name ИмяВиртОкружения -c conda-forge selenium 
                >> в имеющееся окружение с именем "ИмяВиртОкружения":
                    conda install --name ИмяВиртОкружения -c conda-forge selenium
            > в Анаконду (оконный интерфейс)
                >> Anaconda Navigator -> Группа Environments
                    + выберете виртуальную среду или создайте её, скопировав имеющуюся как исходную
                    + добавьте канал (channel) "conda-forge"
                    + (в правой части окна) выполните поиск "selenium"
                    + выберете "selenium" и запустите процесс... applay

        1.1. Selenium Server (Grid)
            https://habr.com/ru/post/248559/
            -- Selenium server необходим в случаях, когда вы хотите использовать remote WebDriver [удаленный — Прим. пер.]. За дополнительной информацией обращайтесь к разделу Использование Selenium с remote WebDriver.
            
        2. WebDriver 
            Внимание! Версия драйвера должна соответствовать версии браузера

            Головная страница:
                https://www.selenium.dev/selenium/docs/api/py/index.html
            
            Chrome:	https://chromedriver.chromium.org/downloads
            Edge:	https://developer.microsoft.com/en-us/microsoft-edge/tools/webdriver/
            Firefox:	https://github.com/mozilla/geckodriver/releases
            Safari:	https://webkit.org/blog/6900/webdriver-support-in-safari-10/


            > FireFox geckodriver, установка:
                >> Anaconda Navigator 
                    -- найти geckodriver и запустить установку
                    -- поиском найти "geckodriver.exe" и путь до него использовать в ...
                    
                        from selenium.webdriver.firefox.service import Service
                        webdriver_firefox_path = r"C:\Programs\Anaconda3\envs\scrapy\Scripts\geckodriver.exe"
                        webdriver_firefox_service = Service(firefox_gekodriver_path)
                        driver = webdriver.Firefox(service=webdriver_firefox_service)
            > Crome 
                >> Руками:
                    https://chromedriver.storage.googleapis.com/index.html?path=108.0.5359.71/

                >> Anaconda Navigator 
                    -- найти по слову Crome...
                        python-chrome (Webdriver for chrome (binary))
                        -> выбрать и установить

                    -- поиском найти "chromedriver.exe"
                        webdriver_chrome_service = chromeService(r"C:\Programs\Anaconda3\envs\scrapy\Lib\site-packages\chromedriver_binary\chromedriver.exe")
                        driver = webdriver.Chrome(service=webdriver_chrome_service)


КУРС
    1. Основы клиент-серверного взаимодействия. Работа с API
       2022-09-19; 20:00 MSK (понедельник)
        - Узнаем основные принципы сбора данных.
        - Как отправлять GET-запросы при помощи разных инструментов.
        - Как работать с ответами от сервера и API, JSON
            Домашнее задание: 
                Сдать до:   2022-09-26; 20:00 MSK
                Сдано:      2022-09-24; 20:20
                Коммент.:   сложно...
            ПРОВЕРКА
                Проверено:  2022-09-25; 08:49 MSK 
                Оценка:     5-ка, ОТЛИЧНО.
                Коммент.:   Добрый день, Олег! Все отработало без ошибок. К оформлению так же вопросов нет. (Эрик Муллагалиев・Преподаватель)

    2. Парсинг данных. HTML, DOM, XPath
       2022-09-22; 20:00 MSK (четверг)
        - XPath, парсим с отладкой прямо в браузере
            Домашнее задание: 
                Сдать до:   2022-09-29; 20:00 MSK
                Сдано:      2022-09-00; 20:20
                Коммент.:   Запросил подсказку по доступу к атрибуту xlink:href 
            ПРОВЕРКА
                Проверено:  2022-09-30; 09:56 MSK 
                Оценка:     5-ка, ОТЛИЧНО 
                Коммент.:   Добрый день, Олег! В конце 4 урока разобрали эту проблему. Это из-за того, что теги html-м не являются. А так, по коду проблем нет, все работает.

    3. Парсинг данных. HTML, Beautiful Soap
       2022-09-26; 20:00 MSK (понедельник)
        - ... надо разобраться...
            Домашнее задание: 
                Сдать до:   2022-10-03; 20:00 MSK, отлжено до 17
                Сдано:      2022-10-09; 22:30
                Коммент.:   Сделан парсинг только одного сайта из 3-х (кажеться)...
            ПРОВЕРКА
                Проверено:  2022-10-10; 21:50
                Оценка:     5-ка, Отлично
                Коммент.:   Эрик Муллагалиев, преподаватель. Добрый день, Олег! Все решено правильно!

    4. Система управления базами данных MongoDB в Python
       2022-09-29; 20:00 MSK (четверг) 
        - С базой данных работать интересно, но тяжеловато по началу...
            Домашнее задание: 
                Сдать до:   2022-10-20; 20:00 MSK
                Сдано:      2022-10-20; 01:30
                Коммент.:   Много мелочей, для работы с MongoDB...
            ПРОВЕРКА
                Проверено:  2022-12-27; 18:53
                Оценка:     отлично
                Коммент.:   все верно выполнено. Можно id сделать как сборный хеш всех полей - тогда проверка на вхождение будет быстрее и эфективнее.
                Ревьювер:   Вадим Мазейко, преподаватель 

    5. Парсинг данных. Scrapy. Начало
       2022-10-03; 20:00 MSK (понедельник) 
        - 
            Домашнее задание: 
                Сдать до:   2022-10-00; 20:00 MSK
                Сдано:      2022-10-00; 00:00
                Коммент.:  
            ПРОВЕРКА
                Проверено:  2023-01-08; 10:59
                Оценка:     отлично
                Коммент.:   Отличная работа!
                Ревьювер:   Вадим Мазейко, преподаватель

    6. Фреймворк Scrapy, pipelines, Splash
       2022-10-06 20:00 MSK (четверг)
        - Очень интересно всё это работает в паучке 
            Домашнее задание: 
                Сдать до:   2022-10-00; 20:00 MSK
                Сдано:      2022-12-14; 00:00
                Коммент.:   так долго всё делал, и то не всё, что хотел (учебная база и hh.ru, splash, pipelines.py, SQLite, MongoDB)
                            не сделал  labirint.ru и/или book24.ru -- сложные динамические сайты
            ПРОВЕРКА
                Проверено:  2023-01-08; 11:08
                Оценка:     отлично
                Коммент.:   Отличная работа!
                Ревьювер:   Вадим Мазейко, преподаватель

    7. Парсинг данных. Selenium в Python
       2022-10-10 20:00 MSK (понедельник)
        - Фантастически здорово управлять сайтом!
            Домашнее задание: 
                Сдать до:   2022-10-00; 20:00 MSK
                Сдано:      2022-12-26; 22:19 
                Коммент.:   Только авторизация пользователя "вручную" на учебном сайте (с ожиданема объектов явным и неявным)
                            ItemLoader -- не решено!
            ПРОВЕРКА
                Проверено:  2023-01-08; 11:12
                Оценка:     отлично
                Коммент.:   --- 
                Ревьювер:   Вадим Мазейко, преподаватель

    8. Фреймворк Scrapy. Реализация механизмов клиент-серверного взаимодействия
       2022-10-13 20:00 MSK (четверг)
        - Совершенно замучился и делал несколько месяцев...
           
            Домашнее задание: 
                Сдать до:   2022-10-00; 20:00 MSK
                Сдано:      2023-03-02; 11:38 (в службу поддержки и они сами прикрепят работу на сайте урока)
                URL:        https://github.com/Pomidoroff-Tomatoff/gbds-02-01-Data_Get_Internet/pull/9
                Коммент.:  (Мой) Домашняя работа состоит из двух частей: тестовая проверка материала из методички и собственно задания.
                            1-ое задание взял из методички (заполнение формы и отправка и получение результата после регистрации на сайте
                            https://www.scrapethissite.com/pages/advanced/?gotcha=csrf
                            -- всё успешно.
                            ВОПРОС по тесту методички: не удалось успешно выполнить тест из методички урока на сайте https://quotes.toscrape.com/login с использованием запросов SplashRequest() (первый запрос) и SplashFormRequest() -- результат (количество найденных цитат = 0, то есть не находим) не получаем. При этом, если первый запрос заменить обычным, то есть Request(), то дальнейшая отправка формы со сплэшем (SplashFormRequest) работает успешно. Почему?
                            2-ое задание с сайта урока (инстаграмм) я заменил сбором данных (паук api\hh_api) с сайта hh.ru по API, сохранением этих данных в БД (MongoDB и SQLite) и некоторый анализ -- выборку (Jupyter блокнот) по условию (уровню зарплаты).
                            Здесь вход на сайт (заполнение и отправка формы) не выполняется.
            ПРОВЕРКА
                Проверено:  2023-03-04; 14:04
                Оценка:     отлично
                Коммент.:   По дз - отличная работа. По реквестам - у меня отработало. Буду пытаться воспроизвести ошибку.
                Ревьювер:   Вадим Мазейко, преподаватель
            ВОПРОСЫ 
                Преподавателю и наставнику о множественных значениях для одного процессора в одном поле: как разспределить значения по другим полям...
    9. СЕРТИФИКАТ получен 2023.03.07.
       October 4, 2021. No 1413614
        RU: https://gb.ru/certificates/1413614.ru
        EN: https://gb.ru/certificates/1413614.en
        Партнёрская ссылка:
            https://gb.ru/go/d86QBn
        
        Итог курса:
            Дата 
                > планового завершения:    2022 осень 
                > фактического завершения: 2023.03.07
            Баллы: 111. 
            Предложено стать наставником.



АНАЛИЗ ДАННЫХ ИЗ ИНТЕРНЕТ (прошлые занятия)
    RFM-анализ. 
        Что это такое?
        RFM аббревиатура (англ. Recency Frequency Monetary — давность, частота, деньги) — сегментация клиентов в анализе сбыта по лояльности. Определяет три группы: Recency (давность) — давность сделки, чем меньше времени прошло с момента последней активности клиента, тем больше вероятность, что он повторит действие.
    Мой поиск:
        https://www.unisender.com/ru/blog/idei/rfm-analiz/


Принципы сбора данных
    Парсинг 
        - Синтаксический анализ HTML-информации необходимый для:
            > поиска
            > выборочное извлечение 
        - Этичный Парсинг
            > API сайта -- лучшее решение (если сайт его предоставляет) 
            > Уважаем robots.txt
            > Условия использования сайта -- принимаем во внимание 
            > Парсим не мешая пользователям: ночью, например, 
            > Парсим с паузой, чтобы не быть похожими на DDoS-атаки
            > Уважительное отношение к сайту и данным: 
                >> брать только то, что нужно
            > Цель получения данных: создавать ещё большие ценности (а не дублировать информацию) 
            > Оставляйте ссылку на сайт-источник, чтобы отплатить сайту за использование егт данных для повышением его трафика по вашим ссылкам
                >> важно это делать правильно 

    Скрапинг (Web Scraping)
        - Общий сбор веб-данных 

    Краулинг (crawling)
        - сканирование с целью обнаружения новых данных и обновлённых страниц
            > переход по ссылкам так же происходит 
        - поисковый бот, паук...
        - Robots.txt 
            > устаревший, для краулинга
            > часть протокола исключения исключения роботовов (REP) 
            > файл владельца сайта для указания краулеру, что можно парсить...
            > Requests -- не учитывает этот файл.
            > Scrapy -- нужно указать, принимать во внимание Robots.txt или нет...



Протоколы и коды 
    HTTP и HTTPS 
        Коды 
        Методы  
        Заголовки 
            имя: значение 
            - 4-е группы

    API
        - публичные
            > отдаются внешним приложениям в их использование
            > прим.: свободные для скачивания библиотеки (прим.: Twitter и Facebook)

            Пример: Python get с сайта github.com при помощи API

                import requests
                user_login_name = 'pomidoroff-tomatoff'
                user_resources = {
                    'repos':    "общедоступные репозитории пользователя",
                    'projects': "проекты пользователя", }
                response = requests.get(f"https://api.github.com/users/{user_login_name}/{r_type}")
                repos = response.json()  # только данные (без заголовов)
                print(repos[0]['name'])
                    -> gb-0-02-1-HTML-CoffeeGrinder

            Пример: запишем json-данные на диск 
                import json  # работа с json-файлами...
                with open('01_hw_API__repositories.json', 'w') as file:
                    json.dump(repos, file) 

            Пример: указание юзер-аргумента
                curl --user-agent "Mozilla/5.0 (Windows NT 6.1; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/105.0.0.0 Safari/537.36" -o "01_hw_API__repositories_curl.json" -J https://api.github.com/users/pomidoroff-tomatoff/repos
                # берём из браузера по адресу: chrome://version/

            Пример: Доступ при помощи команды cURL по API с githum.com 
                curl -J https://api.github.com/users/pomidoroff-tomatoff/repos >"01_hw_API__repositories_curl.json"
            - Вывод на Линукс
                cat  "01_hw_API__repositories_curl.json" | grep -i "\"full_name\":" 
            - Вывод на Виндоус
                type "01_hw_API__repositories_curl.json" | find /i "full_name" 

            Примечание:  Токен github.com 
                Github Token для доступа из приложений:
                    Страничка: https://github.com/settings/tokens
                    Personal access tokens -> Generate new token
                    Примечание: токен нужен для получения доступа к списку не публичных репозиториев или для выполнения более чем 60 запросов в час.

            Пример: hh.ru
                Развёрнутый пример (с SQL) и интерактивной аналитикой для Jupyter-блокнота с выводом html-виджетов (сложната)
                    https://office-menu.ru/python/96-api-hh
                Вакансии (любые) по всем регионам выбранного работодателя (сохраняем в файлы):
                    https://temofeev.ru/info/articles/rabota-s-api-headhunter-pri-pomoshchi-python/
                Быстрый старт:
                    https://habr.com/ru/company/hh/blog/303168/


            Пример (quotes.toscrape.com)
                https://github.com/kelvingakuo/quotes.toscrape.com/blob/master/quotesScraper/spiders/scrapeInfiniteScroll.py

                import scrapy
                import json

                class ScrapeInfiniteScroll(scrapy.Spider):
                    name = 'scrapeinfinitescroll'

                #INFINITE SCROLLING READS CONTENT FROM A JSON FILE. 
                #HERE, THESE ARE SERVED FROM 'http://quotes.toscrape.com/api/quotes?page=n' FOR n>=1
                    quote_url = 'http://quotes.toscrape.com/api/quotes?page='
                    start_urls=[quote_url + "1"]

                    def parse(self,response):
                        data = json.loads(response.body)

                        for item in data.get('quotes', []):
                            yield{
                            'text':item.get('text'),
                            'author':item.get('author',{}).get('name')
                            }
                        if data['has_next']:  # предполагаем, что параметр не забыли установить правильно 
                            nextPage = data['page']+1
                            yield scrapy.Request(self.quote_url + str(nextPage))

        
        - приватные API
            > для служебных нужд межсайтового взаимодействия 
                >> поиск -- отладчик в баузере: искать нужно во вкладке NET (сеть) среди подгружаемых ресурсов 
            > не документированый 

    JSON
        - объект: результат работы с API


        
XPath: HTML, DOM, API
    API JSON
        -- атуальная версия --  5.2 (2017.12.14), 
            > появился API-доступ, можно получать и анализировать данные при помощи 
                >> JavaScript
                >> Python -- библиотека-модуль requests (import request )
                    Пример:
                        import request
                        response = requests.get('https://api.github.com/user', auth=(username, token))  # -- см. выше
                        # для кириллицы, актуально для hh.ru
                        data_ru = response.content.decode()  
    HTML:
        -- атуальная версия --  5.2 (2017.12.14), 
            > в ней появились новые элементы:
                >> тег <video> -- для вставки видеопотока
                >> тег <audio> -- для аудиопотока
                >> CSS -- каскадные таблицы стилей (ссылка на стиль передаётся в атрибуте тега)
            > появился API-доступ, можно получать и анализировать данные при помощи 
                >> см. выше
            > удалены оформительские теги, функции которых взяла на себя CSS:
                >> тег <center>
                >> тег <font>
        -- язык гипертекстовой разметки при помощи 
            https://html5book.ru/examples/html-tags.html
        -- тег: описание элементов
            > тег используется для ограничения элемента (задание начала описания элемента и конеца)
                >> начало: <video>
                >> окончание описания: </video> -- добавляется косая черта перед имененм тега
            > главные теги
                >> <!DOCTYPE> -- объявление HTML документа и его версии
                >> html -- элемент, вкл. всю веб-страницу
                >> head -- головная часть HTML-страницы: в ней описываются метатеги, подключаются таблицы CSS и JavaScript, объявляется title страницы (текст, выводится в заголовке окна браузера)...
                >> body -- тег, внутри которого описывается основное содержимое страницы;
                >> header -- именованный тег, определяющий заглавие страницы;
                >> div -- блок, внутри которого находятся вложенные блоки со всем содержимым страницы.
            > виды тегов
                >> пустые элементы: <area>, <base>, <br>, <col>, <embed>, <hr>, <img>, <input>, <link>, <menuitem>, <meta>, <param>, <source>, <track>, <wbr>.
                >> элементы с неформатированным текстом: <script>, <style>.
                >> элементы, выводящие неформатированный текст: <text>, <title>.
                >> элементы из другого пространства имёт: MathML, SVG.
                >> обычные элементы: все остальные...
            > теги хранят необходимую нам информацию:
                >> поиск данных необходимо связывать с тегами
        -- тег, атрибуты: 
            >> помогают индентифицировать нужные теги, часто они становять уникальными (упрощая поиск)
            >> атрибуты указываются в виде имени и его значения в кавычках (после зн. "=")
                Пример: 
                    <font face="Times, Arial, Courier" size=4> Оформляемый текст </font>
            >> class-атрибут:
                * используется для задания сразу нескольких стилей для одного элемента 
                    Прим.:
                        <li class="col-xs-6 col-sm-4 col-md-3 col-lg-3">
                            <article class="product_pod">...</article>
                        </li>
            >> id-атрибут:
                * описывает уникальный элемент на странице, другого элемента с таким именем не может быть.
                * можно однозначно выбрать нужный элемент при помощи id-имени 
                    Пример:
                        Исходный марштур:
                            <div class="col-sm-8 col-md-9>
                                <div class="page-header adction"...</div>
                                <div id="message">...</div>
                                <div id="promotion">...</div>
                        Доступ к элементу:
                            xpath("//div[@id=message]/")  # указываем поиск по всем узлам, так как такой div только 1-ин на странице
                        
            >> href-атрибут:
                Пример: сайт books.toscrape.com
                    books = response.xpath("//article[@class='product_pod']")
                    for book in books:
                        link = book.xpath(".//h3/a/@href").get()
                        print(link)
                            ->  'catalogue/tipping-the-velvet_999/index.html'

            >> title-атрибут:
                Пример: сайт books.toscrape.com
                    books = response.css("article.product_pod")
                    for book in books:
                        title = book.xpath(".//h3/a/@title").get()
                        print(title)
                            ->  'A Light in the Attic'

            >> value-атрибут:
                * могут меняться, поэтому парсить по ним нужно осторожно
        -- HTML-страница передаётся по протоколу 
            > HTTP в виде обычного текст
            > HTTPS в виде зашифрованого текста 
        -- структура страницы HTML 
        -- Сайты, созданные для того, чтобы их парсили:
            > http://books.toscrape.com/
            > https://toscrape.com/

    DOM (Document Object Model) 
        -- объектная модель документа
        -- дерево узлов, так может быть представлен любой документ, 
           где каждый узел это:
            > элемент
            > атрибут
            > текст, 
            > графический или любой другой объект.
        -- для поиска узлов используется XPath 
        -- узлы связаны между собой отношениями "родительский--дочерний"
        -- стандартизирован международным консорциумом W3C (а то различные браузеры имели несовместимые модели документов (DOM))
        -- Уровни:
            > Level 1: поддержка XML 1.0 и HTML (управление деревом, перемещение по дереву)
                >> без пространства имён
                >> уровень, существовавший ещё до разделения модели на Уровни
            > Level 2: поддержка пространства имён XML и CSS 
            > Level 3: 
                a. DOM Level 3 Core
                b. DOM Level 3 Load and Save
                c. DOM Level 3 XPath
                d. DOM Level 3 Views and Formatting
                e. DOM Level 3 Requirements
                f. DOM Level 3 Validation 

    XPath (XML Path) работает так же с HTML
        https://soltau.ru/index.php/themes/dev/item/413
        https://www.w3.org/TR/2017/REC-xpath-31-20170321/
        -- Библиотека:
            > Расшрирение для браузера: ChroPath
                >> Внимание! 
                   Для работы с локальными файлами необходимо в настройках расширение разрешить эту возможность!
            > Модуль для Python: lxml 
                from lxml import html 
        -- Это ЛОКАТОР: возвращает координаты (или указатель или маркер) на найденный элемент, а не сам элемент, вырезанный из DOM
        -- Источник: Awesome Andrew, «Руководство по поиску элементов с использованием XPath в Selenium Python»
            https://www.awesomeandrew.ru/2021/08/06/руководство-по-поиску-элементов-с-исп/
        -- язык запросов для поиска узлов
        -- Связи:
            > Parent (родитель): у каждого элемента и атрибута есть 1-ин родитель
            > Children (ребёнок): 0, 1 или более детей -- может иметь узел
            > Siblings (братья-сёстры): дочерние узлы с одним родителем
                см. далее "Оси поиска..."
            > Ancestors
            > Descendant

        -- Узлы, 7 типов:
            > элементы 
            > атрибуты 
            > текст 
            > пространство имён
            > инструкции по обработке 
            > комментарии
            > узлы доменов 

        -- Маршруты поиска:
            > Элементы описания маршрута поиска (выбора)
                имя_узла -- выберает все узлы именем "имя_узла"
                /  -- выбирает от корневого узла (если стоит первым в пути)... 
                      все узлы от коревого узла 
                      (может быть только один, не имеет родителя, значением является документ)
                      Пример: необходиму указать элемент от корня -- это 'html'
                        '/html'
                      ВНИМАНИЕ: Если путь начинается с косой черты (/book), то он ВСЕГДА представляет собой абсолютный путь к элементу.
                // -- все узлы [муть: от текущего узла (который соответствует выбору) независимо от того, где он находиться]
                .  -- от текущего узла, на который указывает селектор (selector)
                    Пример:
                        footer = xpath('//footer')
                        email = footer.xpath('./a[@class = "email"]/@href')

                            > выделяем первый <p> в списке братьев от текущего узла,
                    Пример:
                        button = responce.xpath('//nav/)              # ищем блок с навигацией
                        button.xpath('.//li[@class="next"]/a/@href')  # от метки начало блока навигации найти ссылку
                            > Внимание!
                                Если точку не поставить, то будем искать по всему DOM, а не от метки начала блока.
                .. -- выбирает (переходит) на родителя текущего узла 
                @  -- выбирает атрибуты 
                    Пример: получить полную ссылку на следующую страницу
                        start_url = 'http://quotes.toscrape.com/
                        url_local = button.xpath('.//li[@class="next"]/a/@href')  # выберем ссылку (м. потреб. text())
                        if url_local:
                            url = start_url.strip('/') + url_local
                    > @class
                    > @data-* 
                        >> HTML5 расширение 
                            - не индексируется поисковыми роботами
                            - не всё работает в JavaScript
                            - не поддерживается IE10 и более ранними
                        >> видится xpath в Scrapy (это точно)

                        Пример:
                            <article
                                id="electriccars"
                                data-columns="3"
                                data-index-number="12314"
                                data-parent="cars">
                                ...
                            </article>

            > Абсолютные пути
                >> Всегда начинаются от корня, то есть "/"
                   Начинаются от корневого узла и начинается всегда с наклонной черты "/" и является общим единственным корнем дерева.
            > Относительные пути
                // -- все элементы...
                    Пример: 
                        - выберем все элементы с названием "title" и атрибутом "lang".
                        //title[@lang]
                        - ...с атрибутом "lang" со значением "en"
                        //title[@lang='en']
                        - все элементы (тэги) "book", которые включают элемент (тэг) "price" со значением больше 35
                        //book[price>35.00]
            > ПРЕДИКАТЫ -- это атрибуты элемента (в квадратных скобках), 
              исп. для придания специфичности маршруту поиска, 
              служат для сужения диапазона поиска 
                Прим.: все div с атрибутом role, равным 'img'
                    //div[@role = 'img']
                Прим.: все 1-ые (первые) div-ы
                    //div[1] 
                Прим.: все элементы "book", кот. включают эл. "price" с текстом как цифр. зн. больше, чем 35.
                    //book[price>35.00]

            > НЕИЗВЕСТНЫЕ или ЛЮБЫЕ узлы
                >> Подстановочные выражения: * и @*
                Примеры:
                    * -- любой узел
                    @* -- любой атрибут
                    //* -- Все элементы в документе
                    //*[@*] -- Все элементы, имеющие хотя бы один атрибут
                    /div/* -- все дочерние узлы от div начиная от корневого узла
                        Пример: 
                            /bookstore/*
                                > все дочерние элементы от "bookstore" начиная от корневого
                    ./div/* -- все дочерние узлы от div начиная от текущего узла
                        Пример:
                            ./bookstore/*
                                > все дочерние элементы от "bookstore" начиная от начала текущего узла
                    //title[@*] -- все элементы с названием "title", которые имеют хотя бы один отрибут любого вида.
                    .//*[@class="author-title"] -- все элементы от текущего с классом "author-title"

        -- Оси поиска в маршрутах
            https://www.scientecheasy.com/2019/08/xpath-axes.html/
            https://guides.hexlet.io/ru/xpath/

            Оси XPath, такие как ребенок, родитель, предок, следующий и предшествующий, имеют важное значение для технического теста Selenium в любой компании.

            //following-sibling::*
                > следующий далее брат (сестра) элемента, то есть выборка в горизонтальной плосткости
                Пример:
                    dom.xpath('.//div[@id="product_description"]/following-sibling::p[1]/text()')[0]
                    -- выделить сразу после указнного дескриптора <div> 
                       следующий (не в глубь) абзац с тегом <p>
            //following
                > все следующие братья (сёстры), то есть список узлов на этом уровне
                > можно исп., чтобы не укзывать множество промежуточных объектов между узлами
                    Пример: между опорными тегами несколько промежуточных
                        html_str = '''
                        <div class="row header-box" xpath="1">
                            <div class="col-md-8">
                                <h1>
                                    <a href="/" style="text-decoration: none">Quotes to Scrape</a>
                                </h1>
                            </div>
                            <div class="col-md-4">
                                <p>
                                    <a href="/login">Login</a>
                                </p>
                            </div>
                        </div> 
                        '''
                        dom = html.fromstring(html_str)  # кажеться так
                        link = dom.xpath('//div[contains(@class, "header")]//following::a[text()="Login"]/@href')

                        # это можно заменить
                        link_parent = dom.xpath('//div[contains(@class, "header")]')
                        link = link_parent.xpath('.//a[text()="Login"]/@href').get()  # XPath в Scrapy

            //preceding-sibling::*
                > предыдущий элемент от текущего
            //preceding
                > все предыдущие узлы от текущего


        -- Методы и функции XPath 
            https://guides.hexlet.io/ru/xpath/
            https://www.awesomeandrew.ru/2021/08/06/руководство-по-поиску-элементов-с-исп/

            коллекция -- несколько элементов одного уровня с доступом по индексу 
                > отсчёт в индексах начинается с 1-цы (а не с нуля)
                    //div[1]
            text() -- возвращает текстовое содержание элемента
                Пример-1:
                    ul/li/div/p/text() -- текст всех абзацев для каждого списка "ul" 

                Пример-2-1: поиск по тексту тега (со сравнением)
                    <span class="button__text">Найти</span>
                    ​//span[text()='Найти'] 

                Пример-2-2: поиск по тексту тега (со сравнением)
                    <div class="col-md-4" xpath="1">
                        <p><a href="/login">Login</a></p>
                    </div>
                    //a[text() = 'Login']
            last() -- последний элемент коллекции
                Прим.:
                    ul/li/div/p[last()] -- последние абзацы для каждого списка "ul".
            first() -- не предусмотрена, для доступа к первому элементу используйте индекс "1".
            position() -- возвращает позицию элента в множестве (коллекции)
            mod -- остаток от деления 
                Прим.: не чётные элементы
                    ul/li/[position() mod 2 = 1]
                Прим.: чётные элементы
                    ul/li/[position() mod 2 = 0]

            contains() 
                -- находит атрибут, если он принимает одно из значений 
                -- указать можно только одно возможное значение или подстроку
                Пример:
                    src = i.xpath('.//div[contains(@class, "card-big__info")]/*[name()="svg"]/*[name()="use"]/attribute::*')   
                Пример:
                    //a[contains(@title, 'Корзина')]
                    //span[contains(text(),'Найти')]

            starts-with()
                -- используется если нам известна первая часть (начальная подстрока) текстового содержимого 
                   искомого элемента на странице, либо часть значения его атрибута.
                    Пример:
                        //a[starts-with(@title, 'Корзина')]
                        //span[starts-with(text(),'Найти')]
                        //a[text() = 'Login']

        -- Логические операторы OR и AND
            > используются в инструкциях предикатов
                Примеры:
                    //a[@rel='noopener' or @target='_blank']
                    //a[@rel='noopener' and @target='_blank']
                    //a[@rel='noopener' and @target='_blank' and contains(@class, 'home-link_black_yes')]

        -- Операции сравнения
            <
            >
            <=
            >=
            Прим.: элементы списка, начиная с 3-его
                ul/li[position() > 2]
            Прим.: элементы списка, до 3-его (не включая)
                ul/li[position() <= 2]

        -- Методы и функции модуля html 
            html.fromstring(response.text) 
                > преобразовывает html-строку в дерево (структуру) DOM 
                > из этой структуры можно выбирать узлы и значения при помощи xpath()

        -- Парсинг другим э-ов: XML-объектов и атрибутов xlink:href и...
            > XPath их не понимает!!!
            > обращение к тэгу "use" и атрибуту, которых не понимает XPath: 
                >> /*[name()="use"] -- локализация тега с именем "use"
                >> /attribute::* -- любой
                    <svg class="card-big__partners-badge">
                        <use xlink:href="#ui-label_moslenta">
                        </use>
                    </svg>

                    DOM.xpath('.//*[name()="svg"]/*[name()="use"]/attribute::*')
                    DOM.xpath('.//*[name()="use"]/attribute::*')
                    -> '#ui-label_moslenta'
                    DOM.xpath('.//use/attribute::*')

            Атрибут: 
                xlink:href="#ui-label_motor" 

                Доступ к его значению:
                    # i -- элемент списка с новостными блоками...
                    src = i.xpath('.//div[contains(@class, "card-big__info")]/*[name()="svg"]/*[name()="use"]/attribute::*')   

        -- Проблемы
            > Если два тега вложены <span> вложены один в другой и внутенний имеет текст, то:
                >> для поиска нужно указывать только 1-ин тег span
                >> возвращается ДВОЙНОЙ результат из дубликатов в списке

                    Пример: (сайт hh.ru, вакансия https://hh.ru/vacancy/73043533)
                        html = '''
                            <div data-qa="vacancy-company__details" class="vacancy-company-details" xpath="1">
                                <span class="vacancy-company-name">
                                    <a data-qa="vacancy-company-name" class="bloko-link bloko-link_kind-tertiary" href="/employer/87021?hhtmFrom=vacancy">
                                        <span data-qa="bloko-header-2" class="bloko-header-section-2 bloko-header-section-2_lite">
                                            <span>
                                                WILDBERRIES
                                            </span>
                                        </span>
                                    </a>
                                </span>
                                <div class="vacancy-company-icon">
                                    <a target="_blank" class="bloko-link" href="https://feedback.hh.ru/article/details/id/5951">
                                        <span class="vacancy-serp-bage-trusted-employer">
                                        </span>
                                    </a>
                                </div>
                            </div>
                        '''
                        Запросы:
                            dom.xpath('//a[@data-qa="vacancy-company-name"]//text()')
                            dom.xpath('//a[@data-qa="vacancy-company-name"]/span[1]/text()')
                            dom.xpath('//div[contains(@class, "block-employer")]//a[@data-qa="vacancy-company-name"]//text()')
                        Результат один:
                            -> ['WILDBERRIES', 'WILDBERRIES']

                    Пример: https://hh.ru/vacancy/71324858
                        html = '''
                            <span data-qa="bloko-header-2" class="bloko-header-section-2 bloko-header-section-2_lite">
                                ООО&nbsp;
                                <!-- -->
                                <span>
                                    HeadHunter::Analytics/Data Science
                                </span>
                            </span>
                        '''
                        -> ['ООО\xa0',
                            'HeadHunter::Analytics/Data Science',
                            'ООО\xa0',
                            'HeadHunter::Analytics/Data Science']

                

Beautiful Soup v.4, HTML
    Documents
        https://beautiful-soup-4.readthedocs.io/
    Сложно:
        http://wiki.python.su/Документации/BeautifulSoup
    Просто (виды парсеров):
        https://docs-python.ru/packages/paket-beautifulsoup4-python/


    Парсить: библиотека Requests 
    Разбор:  библиотека BeautifulSoup (Python)
             библиотека Rubyful Soup (Ruby) 

    from BeautifulSoup import BeautifulSoup          # Для обработки HTML
    from BeautifulSoup import BeautifulStoneSoup     # Для обработки XML
    import BeautifulSoup                             # Для обработки и того и другого

    Пример начала:
     #  импорт класса BeautifulSoup для работы с HTML 
        from bs4 import BeautifulSoup as bs  
        response = requests.get('http://books.toscrape.com/')  # ответ от сервера (учебный сайт)
        soup = bs(response.content, 'html.parser')  # получаем дерево синтаксического разбора 

        soup = bs(response.content, 'html.parser', multi_valued_attributes=None)
            -- список значений атрибута возвращаются не как список list,
                а одной строка (вид, как в HTML)
                Пример:
                    bs('<p class="c_4 c_2 c_3">Параграф 6</p> <p class="c_1 c_2 c_3">Параграф 7</p>', 'html.parser', multi_valued_attributes=None).p.get_attribute_list('class')
                        -> ['c_4 c_2 c_3']

                    bs('<p class="c_4 c_2 c_3">Параграф 6</p> <p class="c_1 c_2 c_3">Параграф 7</p>', 'html.parser').p.get_attribute_list('class')                        
                        -> ['c_4', 'c_2', 'c_3']
        soup = bs(response.content, 'xml', multi_valued_attributes=class_is_multi)
            -- не работает... ???


    -- сбор данных в файла HTML и XML
        > парсер для синтаксического разбора файлов HTML/XML 
        > написан на Python и поэтому очень медленный 
    -- парсер по умолчанию: 'html.parser'
        > встроенный в библиотеку
        > естественная навигация

    Дерево синтаксического разбора 
        > структуры данных, полученных после синтаксического разбора 
        > объект, является экземпляром класса BeautifulSoup

    НАВИГАЦИЯ 

        Имена тегов
            -- навигация при помощи указания имён тегов
                > возвращает один тег с этим именем 
                > указание имён тегов через точку
                    soup.head.a.p 
                > можно указывать параметры тегов, синтаксис соотв. фильру find_all(), см. ниже
                    soup(name='p', class_='c_1')
                > можно использовать указатели направления движения
                    soup.p.parent


        Указатели направления
            .children
                -- прямые наследники, без рекурсии (углубления)
                -- генератор дочерних элементов
                    soup.children
                        -> <list_iterator at 0x580dd60>
            .descendants
                -- все наследники 
            .parent 
                -- подъём вверх 
                -- генератор
            .parents
                -- все родители 
                -- список 
            .previous_sibling и .next_sibling
                -- движение в одном уровне объектов-братьев-сестёр
                -- генератор 
            .next_element
                -- движение к следующему элементу в глубь
                -- генератор
            .next_elements 
                -- все следующие элементы
                -- список 
            .previous_element
                -- движение к предыдущему элементу в глубь
                -- генератор 
            .previous_elements
                -- движение к предыдущему элементу в глубь
                -- список

        ПОИСК по дереву: фильтры

            .find(name, attrs, recursive, string, **kwargs)
                > возвращает не список, а единственное найденное значение или None 
                > эквивалентно fine_all(limit=1)
                > работает многократный вызов:
                    soup.find(class_='card').find(name='p')
                > ищет вглубину
            .find_all(name, attrs, recursive, string, limit, **kwargs)
                > возвращает список 
                > ищем внутрь
                > name -- имя тега, указание для поиска только по этому имени
                    soup.find_all(name='p')
                    soup.find_all('p')
                > attrs 
                    - словарь вида:
                        {'key': value}
                            value == 'строка, регулярное выражение, функцию или значение True'}
                        >> 'key' -- могуть быть зарезервированные слова:
                            class -- зарезервировано в Python, но можно использовать ярлык class_
                                     в версии начиная с Beautiful Soup 4.1.2 уже работает и без ярлыка class_ 

                                Пример: "ИЛИ"-логика
                                    {'class': ['cs-1', 'cs-2'], ...}
                                        где 'cs-1' и 'cs-2' -- возможные значения атрибута class (логика "или")

                                Пример: "И"-логика НЕ работает всегда!
                                    soup=bs(<p class='cs-1 cs-2'>Параграф 5</p>) найдено здесь не будет!
                                    soup.find_all{'class': 'cs-1 cs-2', ...}  
                                        -> [<p class='cs-1 cs-2'>Параграф 5</p>]
                                    soup.find_all{'class': 'cs-2 cs-1', ...}  
                                        -> []
                                        !!! в обратной последовательности найти не удалось !!!
                                
                                Пример: "И"-логика рабочая -- последовательность значений атрибута не играет роли!
                                        Используем селектор CSS 
                                    soup.select('p.c_2.c_1')  
                                        -> [<p class='cs-1 cs-2'>Параграф 5</p>]

                                Пример: Существование атрибута 
                                    {'class': True, ...}
                                        любое значение атрибута class и сам атрибут должен присутствовать объязательно

                            data* -- зарезервировано в HTML (не )
                            name -- зарезервировано в этой функции для обозначения параметра для указания имени тега 
                                Пример:
                                    soup = BeautifulSoup('<input name="email"/>')
                                    soup.find_all(name='email')  # name здесь параметр для указания имени тега, 
                                        -> []                    # а не значение атрибута
                                    soup.find_all(attrs={'name'='email'})
                                        -> [<input name="email"/>]
                        >> value (значение) может быть:
                            *> строка или список строк 
                                soap.find_all(attrs={'class': ['cs-1', 'cs-2']})
                            *> True -- значит атрибут существует с любым значением
                                soap.find_all(attrs={'class': True})
                            *> регулярное выражение 
                                import re 
                                class_=re.compile("itl")
                                soap.find_all(attrs={'id': re.compile('2')}  # ...и найдём с id=222   
                            *> функция (в ф. передаётся единственный аргумент -- тег)
                                Пример: "И"-логика
                                    def func_and(css_class):
                                        if css_class is not None:
                                            if 'c_3' in css_class and 'c_4' in css_class:
                                                return css_class
                                        return None 

                                    soup_tmp = bs('<p class="c_4 c_2 c_3">Параграф 6</p> <p class="c_1 c_2 c_3">Параграф 7</p>', 'html.parser')
                                    soup_tmp.find_all(class_=func_and)
                                        -> [<p class="c_4 c_2 c_3">Параграф 6</p>]
                                Пример: "НЕТ"-логика
                                    def func_tag_not(tag):
                                        if tag.has_attr('class') and not tag.has_attr('id'):
                                            return True
                                    soup.find_all(name=func_tag_not)

                    - можно перечислять атрибуты-аргументы через запятую, кроме зарезервированных (name, data, class до BS4..)
                        .find_all('a', id='123', class=True)  # поиск по тегам a с атрибутами id... и class...

                > string=value -- для поиска text-строк, вместо тегов 
                    -- поиск строк
                    -- возвращает список найденных строк 
                    -- до Beautiful Soup 4.4.0 этот параметр назывался text (далее появился новый параметр string)
                    где: value (значение) может быть:
                        value="строка, регулярное выражение, список, функция, значение True"
                        >> строка  
                            soup.find_all(string="Параграф 1")
                        >> список 
                            soup.find_all(string=['Параграф 1', 'Параграф 6'])
                        >> True
                            soup.find_all(string=True)
                        >> регулярное выражение
                            soup.find_all(string=re.compile('Пара'))
                                -> ['Параграф 6', 'Параграф 7']
                        >> функция
                            в функцию передаётся значение одного аргумента: найденная строка 

                > limit
                    -- как LIMIT в SQL: ограничивает количество строк вывода (в списке...)

                > recursive
                    recursive=False -- ограничение только прямыми потомками (детьми)
                    recursive=True -- не ограничиваться прямимы потомками, искать далее в глубь 

            .find_parent(name, attrs, string, **kwargs)
                > найти родителя (он в любом случае только один)
                > ищем вверх (наружу, к корню)
            .find_parents(name, attrs, string, limit, **kwargs)
                > найти всех предков
            .find_next_sibling(name, attrs, string, **kwargs)
            .find_next_siblings(name, attrs, string, лимит, **kwargs)                    

            .find_next(name, attrs, recursive, string, **kwargs)
                > метод используются для итерации
            .find_all_next(name, attrs, recursive, string, limit, **kwargs)
                > метод используются для итерации
            .find_previous(name, attrs, string, **kwargs)
                > возвращает объект
                > метод используются для итерации по тегам и строкам, который предшествует в документе
            .find_all_previous(name, attrs, string, limit, **kwargs)
                > возвращает список (list)
                > метод используются для итерации по тегам и строкам, которые предшествуют в документе

        Методы
            tag.has_attr(attr), Boolean
                -- Beautiful Soup's method returns a indicating whether an element contains a specific attribute.has_attr(~)
                    soup.find("p").has_attr("id")

            tag.get(attr), text
                -- внутрь тега: извлечение значения атрибута тега
                Пример: url-адрес из тега (извлечение)
                    link = soup.find(name='a', attrs={'class': 'serp-item__title'}).get('href')

            tag.get_text(sep, strip='True'), Text
                -- извлечь весь текст с тега или html-страницы
                -- аргументы:
                    > sep -- строка-разделитель, который убирается для объединения фрагментов
                    > strip=True -- удаление всех пробелов и "\n" в начале и конце слов строки 
                    > strip=False -- не объединяем много строк в одну 
                        Пример:
                            soup.a.get_text('\n', strip=True)


        Навигация похожа на вызов find_all
            Навигация похожа на вызов find_all(...)
            soup.find_all(string=re.compile('Пара'))
                -> ['Параграф 6', 'Параграф 7']
            soup(string=re.compile('Пара'))
                -> ['Параграф 6', 'Параграф 7']
            Примечание: 
                точки между "soup" и скобками "(...)" не должно быть -- обращаемся как к функции.

        Навигация внутри тега 
            .name -- название тега
                Пример: 
                    from bs4 import BeautifulSoup as bs
                    soup = bs('<p class="c_2 c_3">Параграф 6</p>', 'html.parser')
                    soup.p.name
                        -> 'p' 
            .text -- текст внутри тега 
                Пример: 
                    soup.p.text
                > Иногда возвращает ошибку AttributeError,
                  неизестно, пока, как с этим бороться ???????????

            .string 
                -- доступ к строке навигации для объекта NavigableString
                -- это генератор: для доступа к следующему элементу нужно обратиться ещё раз...
                   или в цикле
                    for i in tag.string 
                        type(i)

                        -> <class 'bs4.element.NavigableString'> 
                        ...
            .contents -- список list тегов, которые обрамляет наш тег (к кот. мы обращаемся) 
            .classname -- не знаю, что это; ошибки не вызывает, но и ничего другого тоже не возвращает...
            .attrs -- список атрибутов объекта-тега в виде словаря
                Пример:
                    bs('<p class="c_4 c_2 c_3">Параграф 6</p> <p class="c_1 c_2 c_3">Параграф 7</p>', 'html.parser').p.attrs
                        -> {'class': ['c_4', 'c_2', 'c_3']}


        Функции, дополнительные    
            re.compile(string="шаблон регулярки")
                -- работа с регулярными выражениями модуля re 
                -- может использоваться для поиска подстроки, например в строке перечисления атрибутов класса
                    Пример:
                        import re 
                        soup.find_all(string=re.compile('Пара'))
                            -> ['Параграф 6', 'Параграф 7']
                Примечание: в этом модуле есть ещё несколько методов для работы с регулярными...


    Атрибуты тега
        class
            Пример: поиск по названию класса "в лоб"
                soup = bs('<p class="c_4 c_2 c_3">Параграф 6</p> <p class="c_1 c_2 c_3">Параграф 7</p>', 'html.parser')
                soup.select('p[class=c_3]')
                    -> []
                soup.select('p[class="c_4 c_2 c_3"]')
                    -> [<p class="c_4 c_2 c_3">Параграф 6</p>]

            Пример: КОНТЕКСТНЫЙ поиск ПОДСТРОКИ в строке с перечислением класссов, исп. "*="
                soup.select('p[class*=c_3')
                    -> [<p class="c_2 c_3">Параграф 6</p>, <p class="c_1 c_2 c_3">Параграф 7</p>]

            Пример: КОНТЕКСТНЫЙ поиск ПОДСТРОКИ в строке с перечислением класссов, исп. "~="
                soup.select('p[class~=c_3')
                    -> [<p class="c_2 c_3">Параграф 6</p>, <p class="c_1 c_2 c_3">Параграф 7</p>]


    СЕЛЕКТОРЫ SCC (начиная с версии 4.7.0)
        https://developer.mozilla.org/en-US/docs/Web/CSS/Attribute_selectors
        https://www.w3.org/TR/selectors/

        select() -- Подходит для этой функции 
            .select('p.c_2.c_1') 

        [attr]
            Представляет элементы с именем атрибута attr

        [attr=value]
            Представляет элементы с именем атрибута attr, значение value которого является точно value

        [attr~=value]
            Представляет элементы с именем атрибута attr, значением value которого является разделенный пробелами список слов, 
            одним из которых является именно value

        [attr*=value]
            Представляет элементы с именем атрибута attr, значение value которых содержит по крайней мере 
            одно вхождение значения в строке. То есть, где угодно вхождение, хоть это часть слова.

        [attr|=value]
            Представляет элементы с именем атрибута attr, значение которого может быть точно value 
            или может начинаться со значения value, за которым следует дефис (U+002D) "-". 
            Он часто используется для сопоставления языковых подкодов.

        [attr^=value]
            Представляет элементы с именем атрибута attr, значение value в которых является приставкой (началом слов) для текущего значения.

        [attr$=value]
            Представляет элементы с именем атрибута attr, значение value в котором является суффиксом (окончанием) текущего значения.

        [attrX=value i]
            Чувствительно к капитализации символов
            X = *, |, и так далее из списка выше

        [attrX=value s]
            НЕ-чувствительно к капитализации символов 



MongoDB 
    https://www.mongodb.com/docs/manual/tutorial/
    Инфо 
        -- NoSQL, документоориентированная СУБД (DBMS)
            > открытый исходный код 
            > не требует схемы таблицы
                >> можно загрузить в новую таблицу (коллекцию) словарь и ключи словаря и будут являтся колонками...
            > JSON -- формат хранения данных,  
                >> данные (документы) не структурированы
                >> каждый документ может иметь свою структуру
                >> в любой момент можно добавлять новые поля 
        -- поддерживает масштабируемость 
        -- между элементами есть связи 
        -- поддерживает:
            > автоключ для поля "_id" (первые 4-е байта этого поля содержат timestamp, см. getTimestamp)
            > индексацию для колонок
            > уникальный ключ

        -- Установка: см. выше...
            > Старт сервера Mongo под Linux:
                brew services start mongodb-community@5.0

    Mongo-shell: mongo-клиент в командной строке 
        Вызов в командной строке:
            mongodb
           или
            mongo -u <username> -p <password> --authenticationDatabase <dbname> 
        show dbs -- вывести список баз данных
            > show dbs
            admin   0.000GB
            auto    0.000GB
            config  0.000GB
            local   0.000GB

        show users 
            -- вывести список пользователей

        use database_name 
            -- переключиться на базу данных "database_name"
               сделать её текущей (по умолчанию)
                Пример:
                    use database_name
                        ->  switched to db database_name
                ВНИМАНИЕ: 
                    Если переключение выполняется на новую базу данных, то она не будет создана до тех пор, 
                    пока туда не будут помещены какие-либо данные.

        db
            -- объекта-ссылка (?) на выбранную БД 
            -- принимает значение после выполения команды use
            Внимание!
            Использовать эту ссылку объязательно при всех командах обращения к текущей базе данных!

        .find() 
            > описание см. ниже
            Пример:
                db.<имя_коллекции>.find()
                    -> ... вывод списка всех документов коллекции в консоли
        .count()
            -- (устаревшее)
            -- подсчёт количества записей 
                Пример:
                    db.<имя_коллекции>.count()
                        -> 916
        .countDocuments()
            -- количество записей в коллекции (таблице)
                Пример:
                    use toscrape_book
                    db.pages.countDocuments()
                        -> 1000
        .estimatedDocumentCount()
            -- расчётное количество...
                Пример:
                    db.pages.estimatedDocumentCount()
                        ->  1000
        .insert()
            -- добавить документ или документы
               > лучше вставлять по одному документу, а не кучей, 
                 так как в случае ошибки остановимся где-то по середине и не сможем вренуться назад

        db.auth("username", "password")
            -- аутентификация в базе
        db.logout()
            -- выход из базы данных

        .insertOne({...}) -- вставка документа в коллекцию
            Прим., вставка док-та в коллекцию "users":
                > db.users.insertOne({'name': "Tom"})
                > db.users.find()
                    ->  { "_id" : ObjectId("5498da1bf83a61f58ef6c6d5"), "name" : "Tom" }

        .getTimestamp()
            https://translated.turbopages.org/proxy_u/en-ru.ru.c98df781-63d1ac21-1e094915-74722d776562/https/stackoverflow.com/questions/6452021/getting-timestamp-from-mongodb-id/27613766#27613766
            -- временная метка содержиться в первых 4-х байтах идентификатора _id d MongoDB, если он сгенерирован автоматически
            -- получить стамп времени из _id 
                > db.users.findOne()._id.getTimestamp()
                    -> ISODate("2014-12-23T02:57:31Z")

        drop() -- удаление колекции
            Пример: создание и удаление коллекции "minivan" в базе данных "auto_db"
                > use auto
                switched to db auto
                > db.minivan.insertOne({...})
                > db.minivan.drop()

        dropDatabase() -- удаление БД 
            Пример: удаление базы данных 
                > use auto
                switched to db auto
                > db  # проверяем
                auto
                >db.dropDatabase()
                { "ok" : 1 }

        createIndex( { "user_id": 1 }, { unique: true } )
            -- создание индекса...
                db.members.createIndex( { "user_id": 1 }, { unique: true } )

    Mongo in Python 
        https://www.mongodb.com/docs/drivers/pymongo/
        https://dev-gang.ru/article/integracija-mongodb-s-python-s-ispolzovaniem-pymongo-9hmv4a77cw/

        -- Импорт модуля для работы с монго
            import pymongo
        -- Подключаемся к серверу баз данных, создавая клиента для "контакта"
            client = pymongo.MongoClient('mongodb://127.0.0.1:27017')
            client = pymongo.MongoClient('mongodb://127.0.0.1', 27017)

        -- Выбор БД выполняется созданием ссылки на неё:
            > нотация с точкой
                db = client.db_name

                Пример: 
                    > запись с точкой...
                        db = client.db_name
                    Внимание! 
                        "db_name" здесь -- это фактическое имя базы данных, а не переменная (с именем базы)

            > нотация без точки -- доступ как к словарю
                db = client["my-illegal-name"] 

            > с исп. переменной db_name
                db_name = "my-illegal-name"
                db = client[db_name]
                
                Пример: 
                    > как словарю (без использования дополнительных функций)
                    с использованием (недопустимой) строки в качестве имени базы данных
                        db = client["my-illegal-name"] 
                    > как словарю при исп. переменной
                        db_name = "my-illegal-name"
                        db = client[db-name] 


        -- Методы Python: операции с базой данных PyMongo из Питона 
            import pymongo  # подгружаемый модуль 

            db.my_collection.drop()
                -- Удаление коллекции
            .drop_database('database_name')
                -- Удаление БД (вначале подключаемся к серверу и этот объект называется у нас client)
            .list_database_names()
                -- Список имён баз данных
            .list_collection_names()
                -- список коллекций (аналог таблиц) в конкретной БД
            .count_documents({query, option}) 
                -- количество документов в коллекции 
                Пример:
                    db = client.hh   # Задаём ссылку на базу данных
                    vacancies = db.vacancies   # Создаём ссылку на коллекцию
                    vacancies.count_documents({}) 
                        -> 1160 
            .create_index([('Link', pymongo.ASCENDING)], unique=True) 
                -- создание индекса для коллекци (до точки)
                -- https://pymongo.readthedocs.io/en/stable/tutorial.html

            .insert_one(doc:dict)
                -- добавление одного документа
                -- удобно контролировать ошибки (по ключу, например)
                    Пример:
                        collection.insert_one({"title": "News Paper"})
                        
            .insert_one(doc:dict).inserted_id
                -- атрибут, id последнего вставленного документа
                    Пример:
                        new_doc = {"title": "News Paper"}
                        id_last = collection.insert_one(new_doc).inserted_id
                            ->  5e4465cfdcbbdc68a6df233f

            .insert_many(doc_list)
                -- добавление нескольких документов кучей!...
                Внимание: 
                  - Если произошла ошибка где-то в середине процесса, то предыдущие действия вставки не отменяются
                    и не понятно, какая часть кучи попала в базу, а какая нет!
                    То есть, исправить эту ошибку -- откатить назад -- будет сложно...
            .update_one()
                -- Обновление документа
                    Пример:
                        new_doc = {"title": "News Paper"}
                        id_last = collection.insert_one(new_doc).inserted_id
                            ->  5e4465cfdcbbdc68a6df233f
                        collection.update_one({'_id': id_last}, {"title": "Газета"})
                        collection.find({'_id': id_last})
                            ->  5e4465cfdcbbdc68a6df233f
                            # Здесь, наверное, д.б. не id, а сам документ (обновлённый)

            .update_many() 
                -- ???
                -- ничего плохого про этму массовую операцию не говорили...
                -- необходимо учитывать возможные ошибки уникальности ключа (-ей)

            .delete_one(query)

            .find({..filter..}, {key: 1})
                -- возвращает КУРСОР на отфильтрованную выборку 
                    где {..filter..} -- параметры фильтра
                        {key: 1} -- список полей, которые надо вывести (???)
                    Пример:
                        cursor = vacancies.find({}, {'_id': 1, 'Name': 1})
                        for i, el in enumerate(cursor, start=1):
                            print(i, el)
                        -> 
                    Пример:
                        db.users.find ({'age': {'$gt' : 30}})
                        -> см. ниже
                -- курсор можно преобразовать в список, но тогда он будет занимать место в оператиной памяти

                Операторы выборки (фильтра)
                    https://metanit.com/nosql/mongodb/2.8.php
                    Условные
                        $eg -- равно
                        $ne -- не равно
                        $gt (больше чем)
                            db.users.find ({'age': {'$gt' : 30}})
                        $lt (меньше чем)
                            db.users.find ({'age': {'$gt' : 30, $lt: 50}})
                        $gte (больше или равно)
                        $lte (меньше или равно)
                        $in определяет массив значений, одно из которых должно иметь поле документа
                            db.users.find ({'age': {'$in' : [22, 32]}})
                        $nin определяет массив значений, которые не должно иметь поле документа

                    Логические
                        $or: соединяет два условия, и документ должен соответствовать одному из этих условий
                            db.users.find ({'name': "Tom", $or : [{'age': 22}, {'languages': 'german'}]})
                        $and: соединяет два условия, и документ должен соответствовать обоим условиям 

                        $not: документ должен НЕ соответствовать условию

                        $nor: соединяет два условия, и документ должен НЕ соответстовать обоим условиям

                    Регулярные выражения 
                        $regex -- задаёт регулярное выражение, которому должно соответствовать значение поля. 
                            Пример:
                            > пусть поле name обязательно содержит "USD":
                            > без учёта регистра $options': 'i'
                                db.users.find ({'name': {'$regex':'USD', $options': 'i'}})

                    $elemMatch -- позволяет выбрать элементы подпадающие под определённые условия, когда эти элементы запрятаны в списках...

                    Пример: вывести для всех документов коллекции только поле name и _id
                        collection.find({}, {'_id': 1, 'Name': 1})

            .update_many()
                -- обновление сразу несколькиз записей (осторожно с этим)
            .update_one({})
                -- обновление записи
                -- Операторы
                    > установка значения
                        $set -- задание нового значения ключа 
                                или добавление новой пары ключ--значение (если таковых нет)
                        $setOnInsert -- только при всавке нового документа -- добавление нового поля и значения
                        $unset -- удаление поля и его значения
                    > числовые операторы
                        $inc -- увеличивает значение на заданную сумму
                        $min -- возвращает минимальное значение
                        $max -- ...максимальное
                        $mul -- умножение значения на заданную величину
                    > прочие 
                        $currentDate -- обновляет значение до текущей даты 
                        $rename -- переименование поля 

                    Пример, обновим поле минимальной зарплаты для уникального ключа 'Link':
                        vacancies.update_one(
                            {'Link': {'$eq': 'https://adsrv.hh.ru/click'}},   # документ: равен...
                            {'$set': {'Maney_min': 10}}                       # новое зн.: поле и его новое значение
                        )
                        vacancies.find_one({'Link': 'https://adsrv.hh.ru/click'})['Maney_min']
                            -> 10
                    Пример, удалим поле Maney_min:
                        vacancies.update_one(
                            {'Link': {'$eq': 'https://adsrv.hh.ru/click'}},   
                            {'$unset': {'Maney_min': 1}}        # нужно указать какое-нибудь значение!
                        )
                        # проверим, обратившись к нему:
                        vacancies.find_one({'Link': 'https://adsrv.hh.ru/click'})['Maney_min']
                            -> KeyError: 'Maney_min'

            .delete_one({..фильтр..})
                -- удаление документа из коллекции, -- первого подходящего под указанные условия
            .delete_many({..filter..})
                Пример, удаление всех документов коллекции, для указанного фильтра:
                    collection.delete_many({})

            .distinct('key_name')
                -- вывести только уникальные значения "заключённого" поля (с именем по ключу)
                    Пример:
                        vacancies.distinct('Maney_curr')
                            ->  ['', 'EUR', 'USD', 'руб.']

            .skip(2)
            .limit(3)

            .min({'name': minValue})
            .max({'name': maxValue})
                -- используются только с индексом
                -- индекс передаётся в метод только при помощи метода .hint()
                    Пример:
                        db.users.createIndex({"age": 1})
                        db.users.find().min({age:30}).hint({age:1})

        Агрегирование
            .aggregate({'$group': {'_id': 1, 'new_key_all': { '$sum': '$price' }}}) 
                -- метод агрегирования
                    где '$price' -- поле 'price', по которому выполняется операция суммирования

            Операторы агрегирования
                $group
                    -- группирует входные документы по указанному выражению-идентификатору 
                       и применяет выражения-аккумуляторы, если они указаны, к каждой группе.
                $sum
                $match
                    -- Фильтрует документы, чтобы передать только те документы, которые соответствуют 
                       заданное условие (условия) для следующего этапа конвейера.

                    Пример:
                        db.cars.aggregate(
                            { '$match': {'$or': [ { 'name': car_first }, { 'name': car_second }] }}, 
                            { '$group': {'_id': 1, 'sum2cars': { '$sum': "$price" } }}
                        )

        Обработка ошибок
            import pymongo
            try:
                ...
            except pymongo.errors.DuplicateKeyError:
                pass  # обработка дубликатов
            except:
                ...
            else: 
                pass



SQLite3
    https://www.sqlite.org/index.html
    sqlite.org

    Установка: 
    -- Сервера не существует в принципе.
    -- Необходимы библиотеки, Python: sqlite3
    -- Может потребоваться клиент, например DBeaver или командой строки

    Скорость: 
        -- Каждая запись при вставке заноситься на диск и проверяется очистка кэша диска для уверенности сохранности записанных данных. В связи с этим скорость записи напрямую зависит от скорости вращения шпинделя жёсткого диска. Например, для каждой транзакции он должен сделать как минимум 2-а оборота диска и при 7200 об. в минуту может быть совершено 60 транзакций в секунду.

    Основные операции
        -- подключение к базе

            import sqlite3
            connection = sqlite3.connect(base_name)
            cursor = self.connection.cursor()

        -- создание таблицы

            create_table_query = f'''
                CREATE TABLE {table_name}(
                    _id INTEGER PRIMARY KEY,
                    title TEXT,
                    salary TEXT,
                    date_publication TEXT
                )
                '''
            cursor.execute(create_table_query)
            connection.commit()

        -- вставка записи в таблицу
        
            insert_query = f'''
                INSERT INTO {table_name}(
                    title,
                    salary,
                    date_publication
                    )
                VALUES(?, ?, ?)
                '''
            cursor.execute(self.insert_query, (
                item.get('title'),
                item.get('salary'),
                item.get('date_publication')
                )
            )
            connection.commit()
            
    
    -- скрипт SQLite: 
        Создание таблицы с ключевым полем:
            CREATE TABLE t1(
                a INTEGER PRIMARY KEY,
                b INTEGER
            );

        Вставка
            INSERT INTO t1 VALUES(NULL, 123);

        Типы данных: 
            -- INTEGER, REAL, TEXT, BLOB или как NULL.
            -- Динамическая типизация



MySQL
    Инфо
        пример: https://python-scripts.com/pymysql
    Установка:
        pip install pymysql
    
    Команды
    -- Подключение к серверу 
        Пример:
            import pymysql 
            connect = pymysql.connect(
                host='localhost',
                password='s$cret',
                db='db_name',
                charset='utf8mb4',
                cursorclass=pymysql.cursors.DictCursor
            )
            with connect:
                cursor = connect.cursor()
                cursor.execute("SELECT VERSION()")  # запуск команды SQL на выполнение 
                version = cursor.fetchone()  # возврат первой строки ответа 
        
        rows = cur.fetchall() -- получение всех строк ответа...



Scrapy 
    Учебник (очень хороший!)
        https://docs.scrapy.org/en/latest/intro/tutorial.html
    Последняя версия:
        https://docs.scrapy.org/en/latest/
    Версия 1.4:
        https://docs.scrapy.org/en/xpath-tutorial/index.html
    Самоучитель:
        https://www.tutorialspoint.com/scrapy/scrapy_item_loaders.htm
    Перевод:
        https://digitology.tech/docs/scrapy/intro/tutorial.html
    Перевод и ОКРУЖЕНИЕ:
        https://konstantinklepikov.github.io/myknowlegebase/notes/scrapy.html

    Установка (см. выше INSTALL):

        Стандартный установщик Питона
            pip install scrapy
        АнаКонда, в указанное окружение
            conda install --name my-Env Пакет
            conda install --name my-Env -c conda-forge scrapy
                > более новая версия, команда не проверена ...
        АнаКонда, в текущее окружение
            conda install scrapy 
            conda install -c conda-forge scrapy
                > последний вариант с каналом conda-forge может установить более новую версию 
    Запуск в косоли:
        activate
            -- включение управления окружениями (в path что-то добавляется...)
        activate scrapy 
            -- включение окружения с именем «scrapy»
        scrapy -h
            -- поехали!
            -- запускаем скрэйпи с подсказкой основных команд

    Настройки:
        https://docs.scrapy.org/en/latest/topics/settings.html#std-setting-LOG_LEVEL
        > глобальные
        > проектные (settings.py)
        > паука (переменная custom_settings = {...})

    Введение (scrapy)
    -- Инструмент сбора данных в интернете с сайтов.
        > HTML, Json (API)
        > 

    -- Фреймворк Python: 
        > строгий набор файлов-шаблонов Python, предназначенных для создания web-пауков, 
        > пауки настраиваются программистом
        > эта система файлов запускается ядром на выполнение
        > Scrapy построен на основе асинхронной сетевой библиотеки Twisted, 
          поэтому вам нужно запустить его внутри реактора Twisted.

    Структура Scrapy кратко
        -- Scrapy Engine 
           > отвечает за координацию между всеми компонентами
        -- Scheduler
           > получает запросы от движка и ставит их в очередь для возврата обратно в движок
           > очердь может быть асинхронной (моё прим.), если такое может быть 
        -- Downloader
           > отвечает за получение веб-страниц
           > запрошенные от движка веб-страницы возращает загруженными движку (через Downloader Middlewares)
        -- Downloader Middlewares 
           > отвечает за загрузку разметки сайта
           > расположен между загрузкиком и ядром 
           > механизм расширения функциональности: место для добавления прикладного кода 
        -- Spiders
           > настраиваемые классы, отредактированные пользователем Scrapy для парсинга данных
           > каждый паук может обрабатывать свой домен или группу доменов 
        -- Spider Middlewares
           > отвечает за возврат данных
           > механизм расширения функциональности: пользовательские заголовки (headers), проксирование...
        -- Item Pipelines 
            > конвеер элементов, финал: настраиваемый(е) класс(ы)
            > данны получены (одна итерация, проход): 
                >> вывод данных наружу: БД, файлы (txt, JSON, CSV)...
                >> к этому моменту синтаксический анализ (парсинг) выполнен 
                >> логическая очистка полученных данных, проверка...

    Инициализация
        инфо: Система файлов-шаблонов:
            > с предопределёнными классами для настройки web-паука(ов) Python
            > создаётся при инициализации проекта ядром 
            > так же файл конфигурации проекта 
            > Тест пути (path) запуска программы spider (windows), чтобы понять откуда мы его запускаем (какой Python или Anaconda)
                where spider 
        Создание, Проект:
            > создание (старт, инициализация)
            > операция в командной строке, папка с проектом "ProjectName" будет создана в текущей папке

                scrapy startproject ProjectName 
                
        Паук: создание
            > создание поискового робота (парсер, синтаксический анализатор)
            > шаблон поискового робота: basic
            > операция выполняется в командной строке в текущей папке "ProjectName" с этим проектом:

                scrapy genspider SpiderName Имя_домена
            где
                SpiderName -- имя паука 
                Имя_домена -- имя домена, которым ограничивается поиск
                    > ограничивает работу скрапи, чтобы не заходить на рекламу (другие домены)
                    > строго без указывания протокола (http, https)
                    > "www" указывать не надо, а только значимые доменные имена (по кот. будет пробегаться паучёк)
                      Пример: books.toscrape.com  # онли

            Пример: создание паучка 
                scrapy genspider books books.toscrape.com

        Паук ++
            > заходим на страницы каждого продукта (полученные из списка продуктов на текущей страницы парсинга)
            > мы не пытаемся собрать краткую информацию о продукте, товаре или книге на имеющейся странице,
              а получив из кратой информации ссылку на страницу товара... переходим на эту страницу товара и получаем полную о нём информацию.

                scrapy genspider -t crawl ИмяПаука Имя_домена
            где
                -t crawl -- шаблон поискового робота "crawl" в модуле ИмяПроекта\spiders\ИмяПаука
                SpiderNamePages -- паук продвинутый (имя)
                Имя_домена -- имя стартового домена, см. выше...

            Пример:
                scrapy genspider quotes quotes.toscrape.com
                    ->  Created spider 'quotes' using template 'basic' in module:
                    ->   splash_quotes.spiders.quotes

        Инструментальные команды Scrapy
            https://docs.scrapy.org/en/latest/topics/commands.html

            Справка:

                scrapy -h
                scrapy <command> -h

            Глобальные command:
                startproject
                    -- создание проекта
                        scrapy startproject <project_name> [project_dir]
                genspider
                    -- создание паука
                        scrapy genspider [-t template] <name> <domain or URL>
                settings
                runspider
                shell
                fetch
                view
                version

            Проектные команды -- только для проекта
                crawl
                    -- запустить паука (обходить, сканировать)
                check
                    -- запустить проверку контрактов
                list
                    -- вывести список имеющихся в проекте пауков
                edit
                    -- доступ к редактору 
                parse
                    -- Извлекает указанный URL-адрес и анализирует его с помощью паука, который его обрабатывает, используя метод, переданный с опцией --callback, или parse, если колбек не указан
                bench
                    -- Быстрый бенчмарк-тест

        Запуск:
            https://docs.scrapy.org/en/latest/intro/tutorial.html

            > Запуск паука на сканирование строго в папке с проектом, вкл. файл scrapy.cfg
              (запуск из скрипта см. ниже)

                scrapy crawl SpiderName [-key key_param]

                где
                    crawl -- команда: сканирование, обход (обходить, сканировать)
                    SpiderName -- имя запускаемого паука
                    key -- необъязательные ключи и параметы этих ключей для вывода (item's) в файл
                        -O -- запись новых данных
                        -o -- дозапись
                        -a -- использование прикладных аргументов паука 
                              см. ниже "Аргументы паука"
                    key_param
                        JSON: 
                            filename.json  -- JSON формат
                                -- запись полученных данных (item's) в файл "filename.json" типа JSON
                        JSON Lines:
                            filename.jl [или filename.jsonlines]
                                -- запись полученных данных (item's) в файл "filename.js" типа "JSON Lines"
                                    >> http://jsonlines.org/
                                    >> каждая запись -- это строка со словарём
                                       много словарей в одну строку...
                                    >> конец записи (документа) -- завершение строки
                                    >> отсутствуют квадратные скобки начала "[" и конца "]" списка в начале и конце файла
                        CSV:
                            filename.csv
                                -- запуск с выводом полученных данных в файл (ключ -o) типа *.CSV

                Пример:
                    scrapy crawl hh_list -o hh_list_items.jl
                    scrapy crawl hh_list -O hh_list_items.json
                    scrapy crawl hh_list -o hh_list_items.csv

            > Запуск автономного паука (без создания проекта)
                spider /?
                scrapy runspider [param] SpiderName 
                -- нужно изучать...
                        
            > Экранный вывод item's выполняется всегда... 
              См. работу с pipelines.py

            > Аргументы паука -- Передача пауку прикладных параметров
              параметры командной строки, передаваемые пауку
                -а -- ключ для передачи параметра (аргумента)
                    >> Данные передаются методу паука __init__ и по умолчанию становятся атрибутами паука
                    >> Передаваемый в прим. параметр tag будет доступен через self.tag
                Пример:
                    scrapy crawl quotes -O quotes-humor.json -a tag=humor
                    
                    def start_requests(self):
                        url = 'http://quotes.toscrape.com/'
                        tag = getattr(self, 'tag', None)
                        if tag is not None:
                            url = url + 'tag/' + tag
                        yield scrapy.Request(url, self.parse)

                    где getattr(self, 'tag', None) -- функция, возвр. значение атрибута 'tag' объекта self, 
                        если этот атрибут существует, а если нет, то третий аргумент None
                        -- это стандартная ф. Питона
                
    НАСТРОЙКИ Scrapy (settings.py):
        Уровни:
            глобальные -- ???
            settings.py -- проект
            custom_settings = {...} -- паук (или модуль (файл) паучка?, то есть не в классе паука, а в файле)

        Общие:

            BOT_NAME = 'books_scrapy'
            -- Имя бота:
                > устанавливается автоматически
                > это лучше не менять...

            FEED_EXPORT_ENCODING = 'UTF-8'
            -- Поддержка Юникода

            USER_AGENT = 'Mozilla/5.0 (Windows NT 6.1; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/106.0.0.0 Safari/537.36'
            -- Агент

            ROBOTSTXT_OBEY = False
            -- Подчинение правилам файла "robots.txt":
                > если в этом файле запрещено парсить страницу, то Scrapy не будет её парсить, 
                чтобы указать Scrapy парсить сайт без его "без разрешения" необходимо игнорировать файл "робот..."
                    # Obey robots.txt rules
                    
        Этичный парсинг: 
            CONCURRENT_REQUESTS = 16
            -- Количество одновременных (параллельных) запросов:
               Configure maximum concurrent requests performed by Scrapy (default: 16)

            Задержки
                DOWNLOAD_DELAY = 0.5 
                -- Задержка между запросами веб-страниц (по умолчанию 0)
                -- Это минимальная задержка!
                    # Configure a delay for requests for the same website (default: 0)
                    # See https://docs.scrapy.org/en/latest/topics/settings.html#download-delay
                    # See also autothrottle settings and docs

                -- АвтоПростой: расширение каждый раз меняет задержку 
                от стартовой (AUTOTHROTTLE_START_DELAY) до минимальной (DOWNLOAD_DELAY) или максимальной
                Enable and configure the AutoThrottle extension (disabled by default)
                    See https://docs.scrapy.org/en/latest/topics/autothrottle.html

                AUTOTHROTTLE_ENABLED = True
                -- АвтоПростой, включение

                AUTOTHROTTLE_START_DELAY = 5
                -- Начальное время простоя, если всё будет хорошо, то будет использоваться DOWNLOAD_DELAY
                The initial download delay

                AUTOTHROTTLE_MAX_DELAY = 60
                -- Максимальное время простоя: если стартового простоя не будет хватать
                The maximum download delay to be set in case of high latencies

                # The average number of requests Scrapy should be sending in parallel to
                # each remote server
                #AUTOTHROTTLE_TARGET_CONCURRENCY = 1.0
                # Enable showing throttling stats for every response received:
                #AUTOTHROTTLE_DEBUG = False

            COOKIES_ENABLED = False
            -- Отключение куки:
                # Disable cookies (enabled by default)

            DEFAULT_REQUEST_HEADERS = {
                'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8',
                'Accept-Language': 'en',
                }
            -- Заголовки
                > скорее всего нужно использовать в самом паучке, модифицирую по мере надобности...
                > Override the default request headers:

            # Crawl responsibly by identifying yourself (and your website) on the user-agent
            #USER_AGENT = 'splash_quotes (+http://www.yourdomain.com)'
            USER_AGENT = 'Mozilla/5.0 (Windows NT 6.1; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/106.0.0.0 Safari/537.36'

        + 
            ITEM_PIPELINES: {
                'books_scrapy.pipelines.BooksScrapyPipeline': 300, # число с приоритеторм
                }
            -- включение в работу классов, объявленных в файле pipelines.ru 
            -- финальная обработка данных (может быть вывод на диск, в базу данных и т.п.)

            LOG_LEVEL = 'DEBUG'
                # https://docs.scrapy.org/en/latest/topics/settings.html#std-setting-LOG_LEVEL
                # In list: CRITICAL, ERROR, WARNING, INFO, DEBUG 

            # Set settings whose default value is deprecated to a future-proof value
            # Установите параметры, значение по умолчанию которых устарело, на значение, пригодное для использования в будущем
            # ВНИМАНИЕ! Если включить этот параметр, то запуск из среды Python останавливается
            #           с ошибкой на строке:
            #           runner.crawl(BTmpSpider)  # -- вот здесь НЕ РАБОТАЕТ!!!
            REQUEST_FINGERPRINTER_IMPLEMENTATION = '2.7'
            TWISTED_REACTOR = 'twisted.internet.asyncioreactor.AsyncioSelectorReactor'

        SPLASH-SCRAPY: настройки для включения плагина
            https://russianblogs.com/article/80881143326/

            Установка Linux Docker под Windows с браузером для проигрывания скрпитов Lua для SPLASH:
                https://dev.to/darksmile92/run-gui-app-in-linux-docker-container-on-windows-host-4kde


            from scrapy_splash import SplashRequest

            SPLASH_URL
                # Сервер splush, размещение !!! 
                # Плагин будет обращаться к серверу Splush именно по этому адресу!!!
                Пример:
                    SPLASH_URL = 'http://localhost:8050',
                    SPLASH_URL = 'http://127.0.0.1:8050/run' 
                        # "run" вроде надо указывать, но в параметроах метода есть 
                        # ключ endpoint='execute'... может он означает то же самое...
                Пример:
                    # Сервер в инете со Splash-ем, случайно найденный...
                    SPLASH_URL = 'https://s1.onekkk.com/'

            SPIDER_MIDDLEWARES = {
                'scrapy_splash.SplashDeduplicateArgsMiddleware': 100,
                }

            DOWNLOADER_MIDDLEWARES = {
                'scrapy_splash.SplashCookiesMiddleware': 723,
                'scrapy_splash.SplashMiddleware': 725,
                'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware': 810,
                }  
                # Downloader Middlewares компоненты, включение
                    > отвечает за загрузку разметки сайта
                    > расположен между загрузкиком и ядром 
                    > механизм расширения функциональности: место для добавления прикладного кода 
                
            DUPEFILTER_CLASS = 'scrapy.dupefilters.RFPDupeFilter'  # Default
                # Двойники
                # Класс, используемый для обнаружения и фильтрации повторяющихся запросов.
                # The class used to detect and filter duplicate requests.

              ? Другое значение: 'scrapy_splash.SplashAwareDupeFilter'
                    > что оно даёт пока не понятно...

                The default () filters based on the REQUEST_FINGERPRINTER_CLASS setting.RFPDupeFilter

                You can disable filtering of duplicate requests by setting DUPEFILTER_CLASS to . Be very careful about this however, because you can get into crawling loops. It’s usually a better idea to set the parameter to on the specific that should not be filtered.'scrapy.dupefilters.BaseDupeFilter'dont_filterTrueRequest

            DUPEFILTER_DEBUG
                Default: False
                By default, only logs the first duplicate request. Setting DUPEFILTER_DEBUG to will make it log all duplicate requests.RFPDupeFilterTrue

                    
            HTTPCACHE_STORAGE = 'scrapy_splash.SplashAwareFSCacheStorage'
                # Класс, реализующий серверную часть хранилища кэша.

    response -- Атрибуты response
        https://docs.scrapy.org/en/latest/topics/request-response.html?highlight=FormRequest.from_response#scrapy.http.Response

        response.url
            > ссылка, по которой был получен ответ

        response.body: html
            > html
                Пример:
                    from scrapy.http import HtmlResponse

                    def parse(self, response: HtmlResponse)
                        file_name = "my_file.html"
                        with open(file_name, 'wb') as f:
                            f.write(response.body)
                        self.log(f'Saved file {file_name}')

        resonse.body: JSON
            > JSON
                response_json = json.loads(response.body)

                Пример:
                    import scrapy
                    import json

                    class QuotesApiSpider(scrapy.Spider):
                        name = 'quotes_api'
                        allowed_domains = ['quotes.toscrape.com']
                        page_flag = True  # существование станицы

                        def start_requests(self):
                            url = 'https://quotes.toscrape.com/api/quotes'
                            page = 0
                            while self.page_flag and (page := page + 1) < 100:
                                request = scrapy.Request(
                                    url = url + f'?page={page}',
                                    callback = self.parse, )
                                yield request
                            return None

                        def parse(self, response):
                            resp_json = json.loads(response.body)       # Получаем JSON из ответа
                            quotes = resp_json.get('quotes')
                            for quote in quotes:
                                item = {
                                    'author': quote.get('author').get('name'),
                                    'tag': quote.get('tags'),
                                    'quotes_text': quote.get('text'),
                                }
                                yield item
                            self.page_flag = resp_json.get('has_next')  # Следующая страница существует?
                            return None


    Методы

        Scrapy-selectors -- Скрапи-селектор -- Скрэйпи-селектор
            https://docs.scrapy.org/en/latest/topics/selectors.html
            https://stackoverflow.com/questions/63859215/using-xpath-with-python-do-i-really-need-to-use-get-or-getall-or-does-the-x

            Инфо
                > Библиотеки доступа к данным веб-страниц
                    >> BeautifulSoup
                        - Синт. анализ HTML, в том числе с плохой разметкой...
                        - Не входит в скрэйпи...
                        - Медленный
                    >> lxml
                        - Синтаксический анализ XML, также анализирует HTML
                        - Не входит в скрэйпи...
                    >> Скрапи-механизм -- Scrapy-selectors, встроенный
                        - XPath для выбора узлов в XML или HTML-документах
                        - CSS -- язык для применения (доступа) к стилей к документам HTML...
                        Внимание!
                            - Селекторы возвращают указатели на данные, а не сами данные. 
                            - Чтобы извлеч непосредственно данные по селектору (указателю на данные)
                              необходимо воспользоваться методами get() или getall()
            
            xpath() -- селектор
                xpath(...).get(default=None) -- получить один элемент из селектора
                xpath(...).getall()          -- список...

                > xpath() возвращает селектор, 
                > чтобы изъять данные по селектору, нужно выполнить get(), см. ниже
                    >> где "default=None" возвращается None (по умолчанию), если возвращать нечего или можно поменять 

                    Пример:
                        from scrapy.selector import Selector
                        body = '<html><body><span>good</span></body></html>'
                        Selector(text=body).xpath('//span/text()').get()
                            ->  'good'
                    Пример: 
                        Selector(text=body).xpath('//div/text()').get(default='not found') 
                            -> 'not found'
                    Пример: список...
                        Selector(text=body).xpath('//span/text()').getall()
                            ->  ['good']

                    Пример: xpath -- текстовый узел
                            - учебный сайт https://docs.scrapy.org/en/latest/_static/selectors-sample1.html
                            - в среде скрэйпи-shell
                        scrapy shell https://docs.scrapy.org/en/latest/_static/selectors-sample1.html
                        >>> response.xpath('//title//text()')
                            ->  [<Selector xpath='//title//text()' data='Example website'>]
                        >>> response.xpath('//title//text()').get()
                            ->  'Example website'

                    Пример: xpath -- атрибуты
                        >>> response.xpath('//div[@id="images"]/*/img/@src')
                            -> [<Selector xpath='//div[@id="images"]/*/img/@src' data='image1_thumb.jpg'>,
                                <Selector xpath='//div[@id="images"]/*/img/@src' data='image2_thumb.jpg'>,
                                <Selector xpath='//div[@id="images"]/*/img/@src' data='image3_thumb.jpg'>,
                                <Selector xpath='//div[@id="images"]/*/img/@src' data='image4_thumb.jpg'>,
                                <Selector xpath='//div[@id="images"]/*/img/@src' data='image5_thumb.jpg'>]
                        >>> response.xpath('//div[@id="images"]/*/img/@src').getall()
                            -> ['image1_thumb.jpg',
                                'image2_thumb.jpg',
                                'image3_thumb.jpg',
                                'image4_thumb.jpg',
                                'image5_thumb.jpg']

                > работа с относительными путями xpath()
                    Пример:
                        divs = response.xpath('//div')

                        абсолютный путь:
                            divs.xpath('//a')
                        относительный путь:
                            divs.xpath('.//a')
                            divs.xpath('a')

            css() -- селектор
                инфо: 
                    стандарт W3C для селекторов css не поддерживает выбор текстовых узлов и значений атрибутов,
                    поэтому Scrapy (parsel) ввёл расширение этого стандарта для внутреннего использования 

                > доступ к ТЕКСТовому УЗЛу в ответе response -- расширение: 
                    ::text

                    Пример: css, текстовый узел
                            - учебный сайт https://docs.scrapy.org/en/latest/_static/selectors-sample1.html
                            - в среде скрэйпи-shell
                        scrapy shell https://docs.scrapy.org/en/latest/_static/selectors-sample1.html

                        >>> response.css('title::text').get()
                            ->  'Example website'
                        >>> response.css('a::text').getall()
                            -> ['Name: My image 1 ',
                                'Name: My image 2 ',
                                'Name: My image 3 ',
                                'Name: My image 4 ',
                                'Name: My image 5 ']

                > доступ к АТРИБУТу в ответе response -- расширение стандарта:
                    ::attr(name)
                        Прим.: name -- имя атрибута -- строго без кавычек

                    Пример: css-селектор -- атрибуты (в shell скрэйпи)
                        >>> response.css('img::attr(src)')
                            -> [<Selector xpath='descendant-or-self::img/@src' data='image1_thumb.jpg'>,
                                <Selector xpath='descendant-or-self::img/@src' data='image2_thumb.jpg'>,
                                <Selector xpath='descendant-or-self::img/@src' data='image3_thumb.jpg'>,
                                <Selector xpath='descendant-or-self::img/@src' data='image4_thumb.jpg'>,
                                <Selector xpath='descendant-or-self::img/@src' data='image5_thumb.jpg'>]
                        >>> response.css('img::attr(src)').getall()
                            -> ['image1_thumb.jpg',
                                'image2_thumb.jpg',
                                'image3_thumb.jpg',
                                'image4_thumb.jpg',
                                'image5_thumb.jpg']

                > доступ к атрибуту объекта-селектора при помощи свойства селектора attrib (словарь, dict):
                    .attrib[]

                    Пример:
                        >>> response.css('img').attrib['src']
                            ->  'image1_thumb.jpg'
                        >>> [img.attrib['src'] for img in response.css('img')]
                            -> ['image1_thumb.jpg',
                                'image2_thumb.jpg',
                                'image3_thumb.jpg',
                                'image4_thumb.jpg',
                                'image5_thumb.jpg']

            urljoin(next_page)
                response.urljoin(next_page)

                > объединение локальной ссылки с основной
                > объединение не требуется, если вместо 
                    scrapy.Request()
                  используется
                    response.follow(a, callback=self.parse)
                    (см. далее)

            Request()
                scrapy.Request(url=next_page_link, callback=self.parse)
                    > запрашиваем загрузка страницы... -- методичка
                Пример:
                    import scrapy
                    from scrapy import Request
                    class MySpider(scrapy.Spider):
                        ...
                        def parse(self, response):
                            ...
                            for href in response.xpath('//a/@href').getall():
                                yield Request(url=response.urljoin(href), callback=self.parse)
                Пример:
                    from w3lib.url import add_or_replace_parameters
                    ...
                        response = scrapy.Request(
                            url=add_or_replace_parameters(self.start_url, self.params),
                            callback=self.parse)
                        
                            
            .follow()
                response.follow(a, callback=self.parse)

                > поддерживает относительные адреса напрямую (в отличии от Request())
                > ссылка или тег а (содержащий ссылку в атрибуте href)
                    >> если используется тег a, то вытаскивать из него (относительную) ссылку 
                       href не надо -- он (метод follow) сделает это сам

                    Пример (длинно):
                        for href in response.css('ul.pager a::attr(href)'):
                            yield response.follow(href, callback=self.parse)
                    Пример (коротко):
                        for a in response.css('ul.pager a'):
                            yield response.follow(a, callback=self.parse)

            .follow_all()
                response.follow_all(a, callback=self.parse)

                > несколько запросов из итерируемого объекта

                    Пример (длинно):
                        anchors = response.css('ul.pager a'):
                        for a in anchors:
                            yield response.follow(a, callback=self.parse)

                    Пример (коротко):
                        anchors = response.css('ul.pager a'):
                        yield from response.follow_all(anchors, callback=self.parse)

            LOG -- Журналирование

                Версия-1. Паук: метод экземпляра класса паука (только)
                    паук.py
                        import scrapy
                        class MySpider(scrapy.Spider):
                            ...
                            self.log() -- запись лога для паука
                            self.logger.info("Visited %s", response.url)

                Версия-2. Библиотека logging
                    любой_файл_проекта.py
                        import logging  # журналирование

                        logger = logging.getLogger(__name__)  # журнал с именени модуля
                        logger.warning(f"Замечание: ...")
                        logger.error(f"Ошибка: ...")
                        logger.info(f"Инфо: ...")

                        class MyPipeline:
                            pass
    
                Внимание!
                    Скрэйпи старается перейти на стандарт журналирования от самого Питона.
                    То есть собственного журналирвоание от класса scrapy.Spider устаревает...


        Методы Python: операции доступа к данным, полученным селекторами

            .get() метод селектора xpath(), Selector
            .extract_first()
                -- используется на объекте ответа (селекторе) 
                   для извлечения 1-ого результата
                -- получение значения атрибута после метода xpath
                Пример:
                    books = response.xpath('//ol[@class="row"]/li')
                    for book in books:
                        item = {
                            'title': 
                                book.xpath('.//h3/a/@title').get()
                            'image': response.urljoin(
                                book.xpath('.//div[@class="image_container"]/a/img/@src').get()),
                            'price': 
                                book.xpath('.//p[@class="price_color"]/text()').get(),
                            'instock': "".join(
                                book.xpath('.//p[contains(@class, "instock")]/text()').getall()).strip(),
                        }
            .get(default='')
                -- Если возвращается None, то вернуть всё равно строку.

            .getall() или 
            .extract()
                -- используется на объекте ответа для получения всех результатов
                -- получение значения атрибута после метода xpath
            .re()
            .re_first()
                -- извлечение данных с помощью регулярных выражений 


    СТРУКТУРА Scrapy подробно

        spider (Паук): Структура basic-паука
            > Атрибуты паука (основные)
                >> name 
                    - адишник, по которому scrapy находит паука
                >> allowed_domains 
                    - список доменов, ограничивающих область поиска
                >> start_urls 
                    - список стартовых урлов краулера
                >> custom_settings - частные настройки краулера, переопределяющие настройки проекта. Есть набор готовых настроек
                    https://docs.scrapy.org/en/latest/topics/settings.html#topics-settings-ref
                >> crawler реализует связку с АПИ scrapi
                    https://docs.scrapy.org/en/latest/topics/api.html#topics-api-crawler
                >> settings настройки паука. Смотри раздел настроек
                    https://docs.scrapy.org/en/latest/topics/settings.html#topics-settings
                >> logger 
                    - обычный python logger, который создается с атрибутом name. 
                      Смотри про логирование в scrapy:
                        https://docs.scrapy.org/en/latest/topics/logging.html#topics-logging-from-spiders
                      Осн. принципы:
                        https://konstantinklepikov.github.io/myknowlegebase/notes/logging

            > Методы специальные -- паука и от паука...
                https://docs.scrapy.org/en/latest/topics/spiders.html

                >> from_crawler(cls, crawler, *args, **kwargs) 
                    - обращение к атрибутам паука до выполнения метода __init__() вроде бы...
                    - @classmethod обязателен!
                    - идея: 
                        1. альтернативный конструктор для экземпляра класса (в pipelines.py, расширения, middleware.py
                           и т.д. -- того класса, куда мы его поместим);
                        2. нужен для передачи от паука атрибутов в конструктор класса __init__(...) в качестве параметров
                        3. атрибуты паука -- это, например, ключи и значения настроек файла sittings.py
                           или ещё какие-нибудь пользовательские данные паучьего класса 
                    Пример:
                        # pipelines.py
                        import pymongo
                        class MongoPipeline:

                            def __init__(self, mongo_uri, mongo_db):
                                self.mongo_uri = mongo_uri  # получаем из метода from_crawler(),
                                self.mongo_db = mongo_db    # который запускается для : передать параметны в __init__()...

                            @classmethod
                            def from_crawler(cls, crawler):
                                ''' 
                                Получим параметры settings и передадим экз-ру через __init__()
                                Что делает return? -- возвращает созданный экземпляр!
                                '''
                                instance = cls(
                                    mongo_uri=crawler.settings.get('MONGO_URI'),
                                    mongo_db=crawler.settings.get('MONGO_DATABASE', 'items')
                                )  # вызываем метод __init__() этого класса, чтобы передать ему параметры для экземпляра...
                                return instance

                        Внимание!
                            Доступ в экземпляре к атрибуту класса можно получить и при помощи атрибута __class__:

                                self.__class__.var = 123

                            так как прямое изменения переменной класса (атрибута) в экземпляре не изменяет переменную класса,
                            но создаёт переменную с таким же именем в экземпляра, которая экранирует классную переменную,
                            после чего изменения переменной класса не будут видны в данном экземпляре:

                                self.var = 555  # не изменит переменную в самом классе, 
                                                # но создаст новую в экземпляре...

                >> set_crawler
                    - устарел...
                >> start_requests() 
                    - сам паук, метод
                    - возвращать итерируемый объект с первым Requests для этого паука. Он вызывается Scrapy, когда паук запускается. Scrapy вызывает его только один раз, поэтому безопасно также реализовать start_requests() в качестве генератора через yield
                >> parse(response) 
                    - сам паук, метод 
                    - колбек по умолчанию, используемый Scrapy для обработки загруженных ответов, когда в их запросах не указан обратный вызов.
                >> log(message[, level, component]) 
                    - Обертка, которая отправляет сообщение журнала через регистратор Spider, нужна для обратной совместимости.
                >> closed(reason) 
                    - вызывается для/при/после закрытия паука (в самом пауке?)
                >> open_spider(self, spider)
                    - # piplines.py
                    - после открытия паука в piplines.py
                    - после спец-методов __init__
                >> close_spider(self, spider)
                    - # piplines.py
                    - после закрытия паука в piplines.py, до вызова специальных методов (__del__ вроде)...
                

            > basic шаблон (создание без ключей по умолчанию)

            Пример: с указанием переменной start_urls со стартовой ссылкой (списка ссылок)
                import scrapy
                from books_scrapy.items import Books_BooksScrapyItem  # импорт классов items.ru, 
                                                                      # для передачи данных от паука дальше... (см. items.ru)
                class BooksSpider(scrapy.Spider):
                    name = 'books'                               # имя паука 
                    allowed_domains = ['books.toscrape.com']     # ограничение по доменным именам -- за них Scrapy выходить не будет (реклама)
                    start_urls = ['https://books.toscrape.com/'] # стартовая страница, СПИСОК ССЫЛОК, по кот. нужно пройтись...

                    custom_settings = {
                        # LOG_LEVEL
                        # https://docs.scrapy.org/en/latest/topics/settings.html  # std-setting-LOG_LEVEL
                        # In list: CRITICAL, ERROR, WARNING, INFO, DEBUG (https://docs.scrapy.org/en/latest/topics/settings.html#std-setting-LOG_LEVEL)
                        'LOG_LEVEL': 'ERROR',

                        # Configure item pipelines
                        # See https://docs.scrapy.org/en/latest/topics/item-pipeline.html
                        # РАЗРЕШАЕМ ИСПОЛЬЗОВАНИЕ piplines.py: точнее классов в pipelines
                        # Мы можем включить НЕСКОЛЬКО классов в файле piplines.py и все эти классы будут задействованы
                        # (будут выполняться объекты этих классов) в соответствии с указанным приоритетом.
                        'ITEM_PIPELINES': {
                            'books_scrapy.pipelines.BooksScrapyPipeline': 300,          # цифра -- это приоритет: он ниже, чем больше цифра
                            'books_scrapy.pipelines.MongoDB_BooksScrapyPipeline': 500,  # заносим в БД MongoDB
                            'books_scrapy.pipelines.TXT_LOG_BooksScrapyPipeline': 600,  # заносим в txt-файл и на экран выводим
                        }
                    }

                    def parse(self, response, **kwargs):

                        # 1. Парсим краткий список книг на текущей странице
                        #    a. Получаем список всех объектов с описанием книг
                        #    б. По этому списку объектов-книг проведём цикл с получением данных по каждой книге 
                        #       и педаём результат дальше по конвейру (yield...)

                        books = response.xpath('//ol[@class="row"]/li')
                        for book in books:
                            yield {'title': book.xpath('.//h3/a/@title').get(),}

                        # 2. Следующая страница (краткого списка книг):
                        #    a. Ищем кнопку "Next" для перехода на следующую страницу и берём из неё локальную ссылку
                        #       на следующую страницу
                        #    б. Если следующая страница есть и локальную ссылку на неё не пустая
                        #       объединяем корневую ссылку с только что полученной локальной ссылкой.
                        #    в. Загружаем новую страницу (методом Request) и результат (response)
                        #       отдаём себе же при помощи асинхронной технологии callback
                        #       для выполнения пункта 1 -- парсинга списка книг.

                        next_page = response.xpath('//li[@class="next"]/a[contains(text(), "next")]/@href').get()
                        if next_page:
                            next_page_link = response.urljoin(next_page)
                            yield scrapy.Request(url=next_page_link, callback=self.parse)

            Пример: генератор запросов start_requests() по ссылкам (они внутри генератора) вместо переменной со ссылками, 
                    возвращаем response при помощи callback сразу в метода парса parse()
                import scrapy

                class BooksSpider(scrapy.Spider):
                    name = 'books'                               # имя паука 
                    allowed_domains = ['books.toscrape.com']     # ограничение доменов

                    def start_requests(self):
                        urls = ['https://quotes.toscrape.com/page/1/', 'https://quotes.toscrape.com/page/2/,]
                        for url in urls:
                            yield scrapy.Request(url=url, callback=self.parse)
                    
                    def parse(self, response):
                        quotes = response.xpath('//div[@class="quote"]')
                        for quote in quotes:
                            yield {'author': quote.xpath('.//small[@class="author"]/text()').get(),}
                        
            

        pipelines.py -- финал конвейера!
            https://docs.scrapy.org/en/latest/topics/item-pipeline.html
            Item Pipelines: структура файла

            > конвеер элементов, финал: вывод данных наружу посредством настройки классов этого файла
            > может быть несколько классов для обработки полученных item
            > на входе получает переменную класса 
                class BooksScrapyItem(scrapy.Item):
                    _id = scrapy.Field()  # key-поле для MongoDB, требуется его задать...
                    title = scrapy.Field()
            > Внимание!
                включение классов, объявленных в pipelines.py, происходит в настройках (см. выше)

            class BooksScrapyPipeline:
                ''' Класс, автоматически созданный при генерации проекта '''

                def open_spider(self, spider):
                    ''' метод, выполняющийся 1-ин раз при открытии паука '''
                    pass

                def close_spider(self, spider):
                    ''' метод, выполняющийся 1-ин раз при закрытии паука '''
                    pass

                def process_item(self, item, spider):
                    return item

        items.py  
            https://docs.scrapy.org/en/latest/topics/items.html

            -- Объявление класса, для описания структуры данных item:
                > похож на словарь

            import scrapy
            class BooksScrapyItem(scrapy.Item):
                ''' структура данных для паучка
                '''
                # define the fields for your item here like:
                _id = scrapy.Field()  # key-поле для MongoDB, требуется его задать...
                title = scrapy.Field()



    Splash -- Scrapy Splash -- плагин Сплэш для Скрэйпи
        https://www.zyte.com/blog/handling-javascript-in-scrapy-with-splash/
        https://splash.readthedocs.io/en/stable/index.html

        -- Рендеринг динамических сайтов, формируемы при помощи JavaScript
        -- См. ниже Работа с формами, примеры...

        Развёртывание Splash (Linux)

            1. Установка:
                sudo docker pull scrapinhub/splash
            2. Запуск:
                sudo docker run -it -p 192.168.0.103:8050:8050 --rm scrapinghub/splash
                    где --rm  ...выгрузить докер после остановки (выхода из консоли)... ???

        Методы Splash:

            from scrapy_splash import SplashRequest, SplashFormRequest

            script = '''
                function main(splash, args)
                    assert(splash:go(args.url))
                    assert(splash:wait(0.5))
                    return {
                        html = splash:html(),}
                end
            '''   # https://splash.readthedocs.io/en/stable/scripting-ref.html

            SplashRequest()
                -- получаем данные

                request = SplashRequest(
                    url=link,
                    endpoint='execute',
                    args={'lua_source': self.script},
                    callback=self.login_form
                )

            SplashFormRequest()
                -- POST (заносим данные)

                def login_form(self, response)
                    form = SplashFormRequest.from_response(
                        response,                       # ссылка для отправки запроса
                        formxpath='//form',             # адрес объекта-формы на странице по xpath
                        formdata={
                            'csrf': csrf_token,
                            'user': username,       # адрес поля с именем и значение
                            'pass': password,       # адрес поля с паролем и значение
                        },
                        args={
                            'lua_source': self.script,
                            'endpoint': 'execute',
                            'wait': 1,
                        },
                        callback=self.after_login
                    )


    Работа с формами
            JsonRequest() -- scrapy.http.JsonRequest
                https://stackoverflow.com/questions/11236632/scrapy-formrequest-sending-json
                -- подкласс запросов так же, как и FormRequest()

                Пример:
                    from scrapy.http import JsonRequest
                    class TestSpider(scrapy.Spider):
                        def start_requests(self):
                            data = {
                                'name1': 'value1',
                                'name2': 'value2',
                            }
                            yield JsonRequest(url='http://www.example.com/post/action', data=data)

            FormRequest() -- Формы ввода данных пользователя 
                https://docs.scrapy.org/en/latest/topics/request-response.html?highlight=FormRequest.from_response#scrapy.http.scrapy.FormRequest.FormRequest.from_response
                https://docs.scrapy.org/en/latest/topics/request-response.html
                https://scrapeops.io/python-scrapy-playbook/scrapy-login-form/

                from scrapy import FormRequest

                -- Заполнение (ввод) и отправка на сервер форм данных пользователя
                -- Виды форм:
                    > Простая

                    > Включающая скрытые данные
                    > Много-этапный процесс входа, требующий много различных данных


                FormRequest(
                    url="http://www.example.com/post/action",
                    formdata={'name': 'John Doe', 'age': '27'},
                    callback=self.after_post
                )

                FormRequest.from_response(
                    response,
                    formdata={'username': 'john', 'password': 'secret'},
                    callback=self.after_login
                )                    
                
                FormRequest.from_response(
                    response,
                    formdata=formdata,
                    clickdata={'name': 'commit'},
                    callback=self.parse1)   
                # https://stackoverflow.com/questions/28038950/how-to-submit-a-form-in-scrapy     

                Пример:
                    import scrapy
                    class QuotesSpider(scrapy.Spider):
                        name = 'quotes'
                        allowed_domains = ['quotes.toscrape.com']
                        start_urls = ['https://quotes.toscrape.com/login']
                        
                        def parse(self, response):
                            csrf_token = response.xpath('//input[@name="csrf_token"]/@value').get()
                            username = 'admin'
                            password = 'admin'

                            form = scrapy.FormRequest.from_response(
                                response,                       # ссылка для отправки запроса
                                formxpath='//form',             # адрес объекта-формы на странице по xpath
                                formdata={
                                    'csrf_token': csrf_token,
                                    'username': username,       # адрес поля с именем и значение
                                    'password': password,       # адрес поля с паролем и значение
                                },
                                callback=self.after_login
                            )
                            yield form
                        
                        def after_login(self, response):
                            quotes = response.xpath('//div[@class="quote"]')
                            print(f"Результат входа на сайт:\n  {response.url=} \n  найдено цитат {len(quotes)=}")
                            return None

                SplashRequest() 
                    См. выше (коротко) Scrapy Splash
                    https://pypi.org/project/scrapy-splash/
                    -- запрос Request() с использованием плагина Splash
                    -- нужен, чтобы получить данные формы
                    -- Куки: скрпт lua для всех запросов через Splash:
                        # Куки...
                        # https://pypi.org/project/scrapy-splash/
                        # For scrapy-splash session handling to work you must use /execute endpoint and a Lua script which accepts ‘cookies’ argument and returns ‘cookies’ field in the result:
                        #   splash:init_cookies(splash.args.cookies)
                        #   return {cookies = splash:get_cookies(), }

                    Пример:

                        import scrapy
                        import scrapy_splash

                        class QuotesJsSplashSpider(scrapy.Spider):
                            name = 'quotes_js_splash'
                            allowed_domains = ['quotes.toscrape.com']
                            start_url = 'https://quotes.toscrape.com/js'

                            custom_settings = {
                                # LOG_LEVEL In list: CRITICAL, ERROR, WARNING, INFO, DEBUG (https://docs.scrapy.org/en/latest/topics/settings.html#std-setting-LOG_LEVEL)
                                # 'LOG_LEVEL': 'DEBUG',

                                # Куки True -- объязательны для задачи login (как правило)
                                'COOKIES_ENABLED': True,

                                'DOWNLOADER_MIDDLEWARES': {
                                    'scrapy_splash.SplashCookiesMiddleware': 723,
                                    'scrapy_splash.SplashMiddleware': 725,
                                    'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware': 810,
                                }                            

                            script = '''
                                function main(splash, args)
                                    splash:init_cookies(splash.args.cookies)
                                    splash.resource_timeout = 1
                                    splash:set_user_agent("Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/98.0.4758.80 Safari/537.36")
                                    assert(splash:go(args.url))
                                    assert(splash:wait(0.5))
                                    return {
                                        html = splash:html(),
                                        png = splash:png(),
                                        har = splash:har(),
                                        cookies = splash:get_cookies(),
                                        url = splash:url(),
                                    }
                                end
                            '''  # https://splash.readthedocs.io/en/stable/scripting-ref.html  # параметры Splash-скрипта

                            def start_requests(self):
                                request = scrapy_splash.SplashRequest(
                                    url=self.start_url,
                                    endpoint='execute',
                                    args={'lua_source': self.script},
                                    callback=self.login_button)
                                yield request

                            def login_button(self, response):
                                # Кнопка Login: получим ссылку и перейдём на страницу (форму) идентификации
                                login_button_link = response.xpath('//div[contains(@class, "header")]//following::a[text()="Login"]/@href').get()
                                # Внимани!
                                # Если сдесь запрос выполнитьс используя SplashRequest(), 
                                # то в итоге parse() не найдёт цитаты этого и следующего примеров...
                                request = response.follow(url=login_button_link, callback=self.login_form)
                                yield request


                SplashFormRequest()
                    # ...
                    def login_form(self, response):
                        csrf_token = response.xpath('//input[@name="csrf_token"]/@value').get()
                        username, password = ('admin', 'admin')

                        form = scrapy_splash.SplashFormRequest.from_response(
                            response,
                            formxpath='//form',             # адрес объекта-формы на странице по xpath
                            formdata={
                                'csrf_token': csrf_token,
                                'username': username,       # адрес поля с именем и значение
                                'password': password,       # адрес поля с паролем и значение
                            },
                            # clickdata={'type': 'submit'},   # https://stackoverflow.com/questions/28038950/how-to-submit-a-form-in-scrapy
                            args={
                                'lua_source': self.script,
                                # endpoint='render.html',   # ??? Что это такое ???
                                'endpoint': 'execute',
                                'wait': 1,
                            },
                            callback=self.parse
                        )
                        yield form

                    def parse(self, response):
                        quotes = response.xpath('//div[@class="quote"]')
                        print(f"Результат после попытки входа на сайт:\n  {response.url=} \n  найдено цитат {len(quotes)=}")
                        return None


                JsonRequest()
                    Sending a JSON POST request with a JSON payload:

                    data = {
                        'name1': 'value1',
                        'name2': 'value2',
                    }
                    yield JsonRequest(url='http://www.example.com/post/action', data=data)

            -- 


    ItemLoader 
        паук.py, 
        item.py
        -- работает с двумя разновидностями входящих данны:
            >> response 
                -- здесь цикла нет...
                    itemloader = ItemLoader(item=BooksItemloaderItem(), response=response)
            >> selector (производная от работы xpath с response)
                for book in response.css("article.product_pod"):
                    itemloader = ItemLoader(item=BooksItemloaderItem(), selector=book)
        Doc: 
            https://docs.scrapy.org/en/latest/topics/loaders.html
        Selector: 
            https://pypi.org/project/itemloaders/

        -- Задаётся в пауке и элементы item загружаются с его помощью
        -- Используется в item.py для пред- и пост-обработки загруженной данных, 
            для получения значений

        Вы можете посмотреть на Item и Itemloader следующим образом: 
        > Item предоставляет контейнер для сохранения захваченных данных
            >> Использование, но подгружаются дополнительные процессоры 
                (которые и выполняют очистку и преобразование поступивших данных)
        > Itemloader предоставляет механизм для заполнения контейнера.
            >> ВНИМАНИЕ: описание и задание непосредственно в файле паука

        class scrapy.loader.ItemLoader(item=None, selector=None, response=None, parent=None, **context) 

            item (scrapy.item.Item)
                -- Имя поля 
                -- Экз. item, заполняемый при помощи add_xpath(), add_css(), add_value()
                -- Объект, который ищем: цена, наименование и так далее...

                    iloader.add_xpath('title', '...')

            selector (объект Selector)
                -- Селектор для извлечения данных (относительно этого селектора)
                -- Представляет из себя указатель на элемент в response, 
                   то есть это сборный объект из ответа (с полными данными) и указателя на элемент (или диапазон) в этом response.
                -- Получается (формируется) методами response.xpath(...) или response.css(...)
                -- Чтобы использовать селектор нужно задавать относительный путь поиска элемента
                   с точкой при описании пути (".//")
                   Пример:
                        iloader.add_xpath('title', './/h3/a/@title')

                   Внимание! 
                        Если относительный путь не использовать (без "."), 
                        то будет выполняться абсолютный поиск по всему response (вероятнее всего) 
                        и вернётся не один найденный товар, а их список.

                -- Удобен, когда на сранице не один товар, а множество, и можно подготовить список из селекторов по этим товарам.
                   Далее работать в цикле по этому списку селекторов на товары.
                    Пример: список однотипных объектов
                        books = response.css("article.product_pod")
                        for book in books:
                            # Ошибка: response должен равняться response, а не book (!)
                            iloader = ItemLoader(item=BooksItemloaderItem(), selector=book)  # <- OK!
                            # относительная ссылка работает и возвращает нужный элемент, 
                            # поиск потокрого ведётся от элемента book (в цикле)
                            iloader.add_xpath('title', './/h3/a/@title')  
                            ...

            response (объект Response)
                -- Противоположность selector
                -- Общий ответ, поступивший пауку, для сбора данных из этого ответа
                -- Используется для поиски индивидуальных объектов, находящихся на странице в единственном экземпляре
                -- Удобен, когда страница посвящена одному изделию (товару) и каждый поиск 
                    в response находит 1-ин элемент (адрес xpath для поиске везде, 
                    без относительных ссылок, начинающихся с точки «.»):

                    iloader = add_xpath('title', '//div[@class="col-sm-8 h1"]/a/text()')

                -- Относительные ссылки для списка элементов, подготовленных селекторм не будут работать,
                    так как находиться будет всегда только первый элемент в группе (относительные ссылки не работаю)
                -- Такой объект ItemLoader не позволяет работать с селектором, то есть со списком элементов, например книг,
                    и поиск выполняект во всём response, а не элементе списка. То есть следующе не работает:
                    Пример: ОШИБКА
                        books = response.css("article.product_pod")
                        for book in books:
                            # Ошибка: response должен равняться response, а не book (!)
                            iloader = ItemLoader(item=BooksItemloaderItem(), response=book)  # <- ОШИБКА!
                            # относительная ссылка, всегда возвратит 1-ый элемент...
                            iloader.add_xpath('title', './/h3/a/@title')  
                            ...

            parent 
                -- ???
            **context
                https://itemloaders.readthedocs.io/en/latest/loaders-context.html#loaders-context

                -- Контекст загрузчика
                -- Произвольный ключ-значение, которые могут передваться вместе с данными для обработки.
                -- Активный в данный момент контекст этого загрузчика Item Loader
                -- Способы здания:
                    > при создании экземпляра загрузчика (собственно)
                        loader = ItemLoader(product, unit='cm')
                    > путём изменения активного текущего контекста в последующем
                        loader = ItemLoader(product)
                        loader.context['unit'] = 'cm'
                    > в объявлении загрузчика элементов (на месте)
                        class ProductLoader(ItemLoader):
                            length_out = MapCompose(parse_length, unit='cm')                            


        Пример-A: обработка ответа RESPONSE (непосредственно)
            ФАЙЛ паука: books.py
                import scrapy
                from scrapy.loader import ItemLoader
                from books_ItemLoader.items import BooksItemloaderItem  # Внимание: в названии папки исп. заглавные буквы!

                class BooksSpider(scrapy.Spider):
                    name = 'books'
                    allowed_domains = ['books.toscrape.com']
                    start_urls = ['https://books.toscrape.com/']

                    def parse(self, response):
                        # парсим заголовок страницы (он один единственный)
                        iloader = ItemLoader(item=BooksItemloaderItem(), response=response)  # ЗДЕСЬ!!!
                        iloader.add_xpath('title', '//div[@class="col-sm-8 h1"]/a/text()')  # полный или абсолютный маршрут
                        item = iloader.load_item()
                        yield item

            ФАЙЛ: item.py
                import scrapy
                from itemloaders.processors import TakeFirst, MapCompose, Join

                def txtup(value: str = None):
                    '''All key make upper. If in value is string.'''
                    if isinstance(value, (str,)):
                        value = value.upper()
                    return value

                class BooksItemloaderItem(scrapy.Item):
                    # define the fields for your item here like:
                    title = scrapy.Field(
                        input_processor=MapCompose(txtup, ),
                        output_processor=TakeFirst()
                    )
            ФАЙЛ: pipeline.py
                class BooksItemloaderPipeline:
                    def __init__(self):
                        self.icount: int = 0

                    def process_item(self, item, spider):
                        self.icount += 1
                        print(f"pipeline: {self.icount:05d} {item=}")
                        return item 

            Результат:
                -> pipeline: 00001 item={'title': 'BOOKS TO SCRAPE'}

        Пример-Б: обработка СЕЛЕКТОРА -- часть RESPONSE -- (response.xpath() или ...css()), в цикле 

            ФАЙЛ паука: books.py
                import scrapy
                from scrapy.loader import ItemLoader
                from books_ItemLoader.items import BooksItemloaderItem  # Внимание: в названии папки исп. заглавные буквы!

                class BooksSpider(scrapy.Spider):
                    ...
                    def parse(self, response):
                        # парсим названия всех книг на странице
                        books = response.css("article.product_pod")
                        for book in books:
                            iloader = ItemLoader(item=BooksItemloaderItem(), selector=book)  # <-- ВОТ!!!
                            iloader.add_xpath('title', './/h3/a/@title')  # относительный маршрут
                            item = iloader.load_item()
                            yield item

            ФАЙЛ: item.py
                # без изменений ...

            ФАЙЛ: pipeline.py
                # без изменений ...

            Результат:
                pipeline: 00001 item={'title': 'A LIGHT IN THE ATTIC'}
                pipeline: 00002 item={'title': 'TIPPING THE VELVET'}
                pipeline: 00003 item={'title': 'SOUMISSION'}
                ...
                pipeline: 00020 item={'title': "IT'S ONLY THE HIMALAYAS"}

        Пример-В: СЕЛЕКТОР (https://pypi.org/project/itemloaders/)
            > устаревшая версия...
            ФАЙЛ: паук
                from itemloaders import ItemLoader
                from parsel import Selector

                html_data = '''
                <!DOCTYPE html>
                <html>
                    <head>
                        <title>Some random product page</title>
                    </head>
                    <body>
                        <div class="product_name">Some random product page</div>
                        <p id="price">$ 100.12</p>
                    </body>
                </html>
                '''
                loader = ItemLoader(selector=Selector(html_data))  # <--!!!
                loader.add_xpath('name', '//div[@class="product_name"]/text()')
                loader.add_xpath('name', '//div[@class="product_title"]/text()')
                loader.add_css('price', '#price::text')
                loader.add_value('last_updated', 'today') # you can also use literal values
                item = loader.load_item()

            Результат:
                    ->  {'name': ['Some random product page'], 'price': ['$ 100.12'], 'last_updated': ['today']}


        Методы ItemLoader
            Основное добавление данных в файле паука spider_имя.py
                .add_xpath()
                .add_css()
                .add_value(field_name, value, *processors, re=None, **kw)
                    -- добавить значение для поля
                    -- где re (str или Pattern) – регулярное выражение, 
                        используемое для извлечения данных из выбранная область CSS

            Замена собранных данных
                .replace_css(field_name, css, *процессоры, re=None, **kw)[источник]
                    -- Аналогично add_css(), но заменяет собранные данные вместо их добавления.
                .replace_value(field_name, значение, *процессоры, re=Нет, **кВт)                
                    -- Аналогично add_value(), но заменяет собранные данные на новое значение вместо того, чтобы добавлять его.
                .replace_xpath(field_name, xpath, *процессоры, re=None, **kw)[источник]¶
                    -- Аналогично add_xpath(), но заменяет собранные данные вместо их добавления.

            Вложенная загрузка (от родительского элемента)
                > задание родительского элемента-селектора, от которого и просходит дальнейшая работа

                .nested_css(css, **context)
                    -- родительска (общая часть) марштура для add_xpath() и add_css()
                .nested_xpath(xpath, **context)
                    -- родительска (общая часть) марштура для add_xpath() и add_css()

                    Пример: парсим элементы собщей частью "footer"...
                        <footer>
                            <a class="social" href="https://facebook.com/whatever">Like Us</a>
                            <a class="social" href="https://twitter.com/whatever">Follow Us</a>
                            <a class="email" href="mailto:whatever@example.com">Email Us</a>
                        </footer>

                        loader = ItemLoader(item=Item())
                        # Загружаем весь footer и устанавливаем селектор на этот родительский объект
                        footer_loader = loader.nested_xpath('//footer')  
                        
                        # От родителя '//footer' ищем дочерние объекты
                        footer_loader.add_xpath('social', 'a[@class = "social"]/@href')
                        footer_loader.add_xpath('email', 'a[@class = "email"]/@href')

                        # ВНИМАНИЕ!
                        # не требудется загрузка footer_loader.load_item()
                        # загружаем всё сразу 
                        loader.load_item()

            Получение данных
                # my_spider.py
                .get_collected_values(field_name)
                    -- Возвращает собранные значения, обработанные с промощью процессоров ввода, для заданного поля.

                .get_css(css, *процессоры, re=None, **kw)
                    -- Возвращает css элемента
                    -- ???
                .get_output_value(field_name)
                    -- Возвращает собранные значения, полученные ранее с помощью процессоров ввода
                    -- Ничего не заполняет и не меняет
                .get_value(value, *processors, re=None, **kw)
                    -- получаем значением, в том числе из JSON
                    -- консоль
                        Пример:
                            from itemloaders import ItemLoader
                            from itemloaders.processors import TakeFirst
                            loader = ItemLoader()
                            loader.get_value('name: foo', TakeFirst(), str.upper, re='name: (.+)')
                                ->  'FOO'
                        Пример:
                            # см. описание процессора SelectJmes ниже...
                            def parse_item(self, json_selector):
                                ''' Непосредственный захват данных
                                    и передача их для очистки в items.py и далее по конвейеру... '''
                                il = ItemLoader(item=HhApiItem(), selector=json_selector)
                                il.add_value('_id', SelectJmes('id')(json_selector))
                                return il.load_item()

                .get_xpath(xpath, *процессоры, re=Нет, **кВт)
                    -- консоль
                    -- Аналогично ItemLoader.get_value(), но получает XPath вместо значение, которое используется для извлечения списка строк Юникода из селектор, связанный с этим ItemLoader.
                        Пример:
                            # HTML snippet: <p class="product-name">Color TV</p>
                            loader.get_xpath('//p[@class="product-name"]')
                            # HTML snippet: <p id="price">the price is $1200</p>
                            loader.get_xpath('//p[@id="price"]', TakeFirst(), re='the price is (.*)')

            Загрузка 
                .load_item()[источник] 
                    -- Заполняет элемент (item) данными, собранными до сих пор, и возвращает его. 
                        Собранные данные сначала передаются через выходные процессоры, 
                        чтобы получить окончательное значение для присвоения каждому полю элемента.

                        Пример:
                            ...
                            loader.add_value('last_updated', 'today') # you can also use literal values
                            item = loader.load_item()
                            yield item 

            ПРОЦЕССОРЫ (файл item.py)
                -- самая ИНТЕРЕСНАЯ часть этой технологии (Item Loader - технологии)
                    > возволяет вести обработку полученных данных не в пауке, 
                        который должен заниматься сбором данных из сети,
                        а файле item.py (это отдельный процесс?)
                        ... 
                    > удобно  
                    > разгрузает паучка
                    > позволяет перечислять обработчики (как из коробки, так и свои) через запятую в процессоре...

                    Пример: все строчные символы в названии переведём в заглавные

                        # item.py
                        import scrapy
                        from itemloaders.processors import MapCompose

                        def txtup(value: str = None):
                            '''All key make upper. If in value is string.'''
                            return value.upper() if isinstance(value, (str,)) else value

                        class BooksItemloaderItem(scrapy.Item):
                            # define the fields for your item here like:
                            title = scrapy.Field(
                                input_processor=MapCompose(txtup, ),  # <- вот это место!!!
                                output_processor=TakeFirst()          # если списко, то первый элемент только

                    Пример: перечисление полей в item
                        # pipelines.py
                        class DefaultValuesPipeline(object):
                            def process_item(self, item, spider):
                                for field in item.fields:
                                    item.setdefault(field, 'NULL')
                            return item
                        или ...
                                for field, value in item.items():
                                    item.setdefault(field, 'NULL')



                Identity
                    -- возвращает исходное значение без изменений
                    -- класс scrapy.loader.processors.Identity
                        Пример:
                            from scrapy.loader.processors import Identity
                            proc = Identity()
                            proc(['a', 'b', 'c'])
                                ->  ['a', 'b', 'c']
                TakeFirst                
                    -- возвращает первое непустое (ненулевое) значение из списка
                    -- класс scrapy.loader.processors.TakeFirst
                        Пример:
                            from scrapy.loader.processors import TakeFirst
                            proc = TakeFirst()
                            proc(['', 'a', 'b', 'c'])
                                ->  'a'

                Join
                    -- соединение при помощи разделителя separate="str"
                    -- класс scrapy.loader.processors.Join (separate="str")
                        Пример:
                            from scrapy.loader.processors import Join
                            proc = Join()
                            proc(['a', 'b', 'c'])
                                ->  'a b c'
                            proc = Join('<br>')
                            proc(['a', 'b', 'c'])
                                ->  'a<br>b<br>c'
                        Пример:

                Compose
                    -- обработка списка значений конвейером функций
                    -- конвейер функций применяется сразу ко всему списку
                        Пример:
                            соединение разделённого на слова абзаца
                    -- каждая функция на вход получает сразу весь списоке значений
                    -- итерационного процесса по списку нет 
                    -- класс scrapy.loader.processors.Compose(*my_func, **default_loader_context)
                        Пример: 
                            # item.py или терминал 
                            >>> from scrapy.loader.processors import Compose
                            >>> proc = Compose(lambda v: v[0], str.upper)
                            >>> proc(['python', 'scrapy'])
                            'PYTHON'

                MapCompose
                    -- обработка каждого значения индивидуально -- всем конвейером функций
                    -- конвейер функций применяется к каждому значению списка индивидуально
                    -- итерационный процесс
                    -- класс scrapy.loader.processors.MapCompose(* функции, ** default_loader_context)
                        Пример:
                            # item.py или терминал
                            from itemloaders.processors import Join, MapCompose, TakeFirst

                            def txtup(value: str = None):
                                '''Все символы заглавными'''
                                return value.upper() if isinstance(value, (str,)) else value

                            class BooksItemloaderItem(scrapy.Item):
                                # define the fields for your item here like:
                                title = scrapy.Field(
                                    input_processor=MapCompose(txtup, ),
                                    output_processor=TakeFirst())

                SelectJmes(json_path)
                    -- JSON-обработка 
                    -- возвращает значение JSON-словаря по ключу
                    -- класс itemloaders.processors.SelectJmes
                        Внимание! 
                            устарел класс scrapy.loader.processors.SelectJmes(json_key)

                        Пример: терминал
                            from scrapy.loader.processors import SelectJmes, Compose, MapCompose
                            
                            proc = SelectJmes("hello")
                            proc({'hello': 'scrapy'})
                                ->  'scrapy'
                            SelectJmes("hello")({'hello': 'scrapy'})
                                ->  'scrapy'
                            proc({'hello': {'scrapy': 'world'}})
                                ->  {'scrapy': 'world'}


                        Пример: паук.py
                            import scrapy
                            import json
                            from scrapy.loader import ItemLoader
                            from scrapy.loader.processors import SelectJmes
                            from api.items import HhApiItem  # пользовательский класс с полями item...

                            class HhApiSpider(scrapy.Spider):
                                name = 'hh_api'
                                allowed_domains = ['api.hh.ru']
                                start_urls = ['https://api.hh.ru/vacancies']

                                def parse(self, response):
                                    json_response = json.loads(response.body)
                                    items = json_response.get('items')
                                    for item in items:
                                        il = ItemLoader(item=HhApiItem(), selector=item)
                                        il.add_value('_id', SelectJmes('id')(item))
                                        il.add_value('name', SelectJmes('name')(item))
                                        yield il.load_item()                        


        ItemAdapter 
            -- что это?
               адаптация формата item (date, напр.) к формату вывода (txt в БД)?
            https://konstantinklepikov.github.io/myknowlegebase/notes/scrapy.html
            -- Строчка в pipeline.py с указанием, что это удобно для различных типов данных
            -- Адаптация данных item к возможностям вывода (сериализатора),
               например, MongoDB не поддерживает тип date... и все преобразования можно сделать одной командой.
            -- Видимо, адаптер нужен для описания типов данных

               Пример:
                # pipelines.py
                import pymongo
                from itemadapter import ItemAdapter

                class MyPipeline:
                    ...
                    self.mongodb_collection.insert_one(ItemAdapter(item).asdict())  # адаптируем данные item под словарь, если он вкл. классы...
                
                Тогда не нужно выполнять, напр., преобразовании типа для поля даты к тексту:
                    item['date_publication'] = str(item.get('date_publication').date())

    API hh.ru
        https://office-menu.ru/python/96-api-hh
        # Параметры GET-запроса
            def getPage(page = 0):
                """
                Создаем метод для получения страницы со списком вакансий.
                Аргументы:
                    page - Индекс страницы, начинается с 0. Значение по умолчанию 0, т.е. первая страница
                """
                
                # Справочник для параметров GET-запроса
                params = {
                    'text': 'NAME:Аналитик', # Текст фильтра. В имени должно быть слово "Аналитик"
                    'area': 1, # Поиск ощуществляется по вакансиям города Москва
                    'page': page, # Индекс страницы поиска на HH
                    'per_page': 100 # Кол-во вакансий на 1 странице
                }
                req = requests.get('https://api.hh.ru/vacancies', params) # Посылаем запрос к API
                data = req.content.decode() # Декодируем его ответ, чтобы Кириллица отображалась корректно
                req.close()
                return data        


    Run -- Run Scrapy from a script -- Запуск паука(ов) из скрипта 
        https://docs.scrapy.org/en/latest/topics/practices.html
        https://docs.huihoo.com/scrapy/1.0/topics/practices.html
        https://question-it.com/questions/12657192/kak-zapustit-scrapy-iz-skripta-python

        -- Crawler API
        -- Scrapy построен на основе асинхронной сетевой библиотеки Twisted, 
           поэтому вам нужно запустить его внутри реактора Twisted.
        -- Виды:
            > CrawlerProcess -- несколько пауков в одном процессе
              https://docs.scrapy.org/en/latest/topics/practices.html#running-multiple-spiders-in-the-same-process
            > CrawlerRunner запуск паука в готовом "реакторе" -- требует дополнительного конфигурирования самого реактора. Все так-же в одном процессе
              https://docs.scrapy.org/en/latest/topics/practices.html#run-from-script

        -- Можно использовать API для запуска Scrapy из скрипта 
           вместо обычного способа запуска Scrapy с помощью 
              scrapy crawl my_spider_name
        -- Невозможно остановить паука CTRL-C
        -- reactor
            Внимание!
            settings.py, 
                необходимо отключить строчку (последняя в файле по умолчанию):
                    TWISTED_REACTOR = 'twisted.internet.asyncioreactor.AsyncioSelectorReactor'
                можно вкл. из подсказки ошибки:
                    TWISTED_REACTOR = 'twisted.internet.selectreactor.SelectReactor'
        -- process (CrawlerProcess)
            >> не выводит логи...

        Пример-А-1: один паук (CrawlerRunner)
            from twisted.internet import reactor
            import scrapy
            from scrapy.crawler import CrawlerRunner
            from scrapy.utils.log import configure_logging

            class MySpider(scrapy.Spider):
                # Your spider definition
                ...

            configure_logging({'LOG_FORMAT': '%(levelname)s: %(message)s'})
            runner = CrawlerRunner()
            d = runner.crawl(MySpider, "NewSpiderName", categories='Travel,Classics', key2='toscrape.com')
            d.addBoth(lambda _: reactor.stop())
            reactor.run()  # the script will block here until the crawling is finished

        Пример-А-2: несколько пауков (CrawlerRunner)
            https://docs.huihoo.com/scrapy/1.0/topics/practices.html#run-from-script
            ...
            configure_logging()
            runner = CrawlerRunner()
            runner.crawl(MySpider1, "NewSpiderName", categories='Travel,Classics', key2='toscrape.com')
            runner.crawl(MySpider2)
            d = runner.join()
            d.addBoth(lambda _: reactor.stop())
            reactor.run()  # the script will block here until the crawling is finished

        Пример-Б-1: ОДИН паук (CrawlerProcess): 
            # не требует откл. последних строк settings.py
            # можно объявить дополнительные настройки
            from scrapy.crawler import CrawlerProcess
            from scrapy.utils.log import configure_logging
            from scrapy.utils.project import get_project_settings
            from books_ItemLoader.spiders.books import BooksSpider  # Паук

            configure_logging()
            settings = get_project_settings()
            # добавляем настройки...
            process = CrawlerProcess(settings, {'USER_AGENT': 'Mozilla/4.0 (compatible; MSIE 7.0; Windows NT 5.1)'})

            process.crawl(BooksSpider, "NewSpiderName", categories='Travel,Classics', key2='toscrape.com'))
            process.start()  # the script will block here until the crawling is finished

        Пример-Б-1: несколько пауков (подпроцессов?) в одном процессе (CrawlerProcess)
            import scrapy
            from scrapy.crawler import CrawlerProcess

            class MySpider1(scrapy.Spider):
                # Your first spider definition
                ...

            class MySpider2(scrapy.Spider):
                # Your second spider definition
                ...

            process = CrawlerProcess()
            process.crawl(MySpider1)
            process.crawl(MySpider2)
            process.start() # the script will block here until all crawling jobs are finished

        Пример-В: Thread
            ... пример не доведён до конца: не известно, как выходить из процесса, 
                если паук завершил свою работу... кроме принудительной остановки по таймауту

            Thread(target=reactor.run, args=(False,)).start()
            time.sleep(5)                                      # Таймаут (задержка) 
            Thread(target=reactor.stop).start()                # Принудительное закрытие


    API -- JSON
        -- Чтобы посмотреть полученные результаты API-запросов необходимо включить в отладчике браузера
            > Вкладку СЕТЬ
            > XHR -- только API-запросы
            > Ответ должен быть таким:
                >> Общие
                    Метод запроса: GET
                    Код статуса: 200
                >> В заголовках ответов получим:
                    Content-Type: application/json

        И мы получили JSON какой-то...

        -- Можно пользоваться документацией по API, если он открытый

        Пример:
            import scrapy
            import json

            class QuotesApiSpider(scrapy.Spider):
                name = 'quotes_api'
                allowed_domains = ['quotes.toscrape.com']
                page_flag = True

                custom_settings = {
                    'ROBOTSTXT_OBEY': False,
                    'TWISTED_REACTOR': 'twisted.internet.selectreactor.SelectReactor' }

                def start_requests(self):
                    url = 'https://quotes.toscrape.com/api/quotes'
                    page = 0
                    while self.page_flag and (page := page + 1) < 100:
                        request = scrapy.Request(
                            url = url + f'?page={page}',
                            callback = self.parse,)
                        yield request
                    return None

                def parse(self, response):
                    resp_json = json.loads(response.body)       # Получаем JSON из ответа
                    quotes = resp_json.get('quotes')
                    for quote in quotes:
                        item = {
                            'author': quote.get('author').get('name'),
                            'tag': quote.get('tags'),
                            'quotes_text': quote.get('text'), }
                        yield item
                    self.page_flag = resp_json.get('has_next')  # Следующая страница существует?
                    return None



    Аргументы паука -- spider command line -- коммандная строка паука
        https://docs.scrapy.org/en/latest/intro/tutorial.html
        -- Внимание!
           Список именованных аргументов команды запуска должен соответствовать принимающей стороне, -- пауку.
           То есть должен соотв. списку именованных аргументов паука (__init__), 
           иначе может получиться большая неразбериха и ошибки.

        -- Аргументы паука передаются через команду crawl с использованием параметра -a 

            Приммер: 
            Shell-команда запуска паука -- Командная строка паука 

                scrapy crawl quotes -O quotes-humor.json -a tag1=humor -a tag2=love

                import scrapy 
                class QuotesSpider(scrapy.Spider):
                    name = "quotes"

                    def start_requests(self):
                        url = 'https://quotes.toscrape.com/'
                        tag = getattr(self, 'tag', None)  # получение аргумента ком. строки tag (значение аргумента)
                        if tag is not None:
                            url = url + 'tag/' + tag
                        yield scrapy.Request(url, self.parse)

        -- По умолчанию метод __init__ принимает любые аргументы паука 
            Пример: 
                Обработка аргументов паука в методе __init__

                class MySpider(scrapy.Spider):
                    name = 'myspider'

                    def __init__(self, my_category=None, *args, **kwargs):
                        super(MySpider, self).__init__(*args, **kwargs)
                        self.start_urls = [f'http://www.example.com/categories/{my_category}']
                        # ...
        -- Метод __init__ по умолчанию полученные аргументы сохраняет как атрибуты паука
            Пример:
                Обработка аргументов в методе start_requests

                class MySpider(scrapy.Spider):
                    name = 'myspider'

                    def start_requests(self):
                        yield scrapy.Request(f'http://www.example.com/categories/{self.my_category}')

            > Внимание!
                Атрибут start_urls (а это список) так изменить нельзя, так как ссылка превратиться в посимвольный список...
                Для решения этой задачи нужно самостоятельно анализировать полученные аргументы. 
                Может быть использовать для этого 
                >> ast.literal_eval() 
                    https://digitology.tech/docs/python_3/library/ast.html#ast.literal_eval
                >> json.loads() 
                    https://digitology.tech/docs/python_3/library/json.html#json.loads
                а затем установить как атрибут.

        -- Передача аргументов из скрипта запуска (runner.py)
            > задаются при определении паука (его имени и, тогда, параметры)
                Пример:
                    # runner.py (стартер: CrawlerRunner + reactor) 
                    ...
                        # аргумент паука my_category:
                        runner = CrawlerRunner(settings)
                        deferred = runner.crawl(BooksSpider, my_category='Travel')
                        deferred.addBoth(lambda _: reactor.stop())
                        reactor.run()

                    # паук...
                    ... анализируем аргумент либо в м. __init__() 
                        def __init__(self, my_category=None, *args, **kwargs):
                            super(MySpider, self).__init__(*args, **kwargs)
                            self.start_urls = [f'http://www.example.com/categories/{my_category}']

                    ... или уже используем start_requests()...
                        def start_requests(self):
                            yield scrapy.Request(f'http://www.example.com/categories/{self.my_category}')

        -- Внимание!
            Список именованных атрибутов скрипта должен соотв. списку именованных атрибутов паука

        -- Полный работающий пример
            Пример: (работающий пример)
                # runner.py
                    import scrapy
                    from scrapy.crawler import CrawlerProcess
                    from scrapy.utils.log import configure_logging
                    from scrapy.utils.project import get_project_settings
                    # Паук:
                    from books_ItemLoader.spiders.books import BooksSpider

                    configure_logging()
                    settings = get_project_settings()
                    process = CrawlerProcess(settings)  # = CrawlerProcess(settings, {'USER_AGENT': 'Mozilla/4.0 (compatible; MSIE 7.0; Windows NT 5.1)'})

                    # Параметры процесса
                    # Внимание!
                    # -- первый неключевой аргумент (MyBook_NewSpiderName) переопределит 
                    #    имя паука в атрибуте экземпляра self.name -- имя станет как в параметре args (первый неключевой)
                    process.crawl(BooksSpider, "MyBook_NewSpiderName", categories='Travel,Classics', key2='toscrape.com')

                    # Запускаем процесс
                    # -- the script will block here until the crawling is finished
                    process.start()

                # book.py (паук)
                    import scrapy

                    class BooksSpider(scrapy.Spider):
                        name = 'books'
                        allowed_domains = ['books.toscrape.com']
                        start_urls = ['https://books.toscrape.com/']

                        def __init__(self, *args, **kwargs):
                            super(BooksSpider, self).__init__(*args, **kwargs)
                            self.icount = 0
                            if args:
                                # Изменение в экземпляре имя паука, заданное атрибутом класса name, 
                                # на новое, полученное от *args
                                print(f"Внимание! Изменено имя паука, теперь: {self.name=} первым неключевым аргументом \"{args=}\"")
                                # self.name = 'books'  # возврат имени паука, если оно было изменено в операторе с параметром *args
                                # print(f"Возвращаем имя паука как в классе: {self.name=}")
                                self.args = args  # задание атрибута класса для parse()

                            if not kwargs:
                                pass
                                # raise "Нет аргументов"
                            else:
                                for key, value in kwargs.items():
                                    value_lst = list(value.split(','))
                                    if len(value_lst) > 1:
                                        kwargs[key] = value_lst
                                self.kwargs = kwargs  # задание атрибута экз. класса для parse()
                                pass

                            print(f"Паук __init__:  {self.name=}: аргументы {args=}")
                            print(f"Паук __init__:  {self.name=}: аргументы {kwargs=}")

                        def parse(self, response):
                            print(f"Парсер, получены аргументы:\n{self.args=}\n{self.kwargs=}")
                            return None

        -- Из скрипта shell-командой
            Пример: (books -- имя паука)
                import sys
                from scrapy.cmdline import execute

                if __name__ == '__main__':
                    sys.argv = 'scrapy crawl books -a key1=Travel -a key2=Classics'.split()
                    execute()

                    # Внимание!
                    # дальнейшие команды в скрипте не выполняются...
                # или (для подготовки списка) с одним ключом и несколькими значениями (без пробелов, строго)
                    sys.argv = 'scrapy crawl books -a key=Travel,Classics'.split()
    (end Scrapy)


Scrapy + Selenium
    https://pypi.org/project/scrapy-selenium-middleware/

    Примеры, в том числе и Selenium:
        https://scrapeops.io/python-scrapy-playbook/scrapy-javascript-rendering-guide/

    -- Что за зверь?... Надо разбираться!


Selenium в Python
    https://www.selenium.dev/documentation/

    -- Инструмент для автоматизации действий веб-браузера: 
        > моделирование команд пользоователя из Python...
        > проверки ссылок 
        > парсинг: работает с сайтами, которые активно этому сопротивляются
        > инструмент для тестирования в своём изначальном виде 
    -- Продукты в рамках Selenium:
        > Selenium WebDriver, 
            >> это компактный объектно-ориентированный API.
            >> библиотека доступна для управления браузерами: FireFox, Chrom, IE, Edge, Opera
            >> вызывает независимую копию браузера со своей историей и профилем...
        > Selenium RC, 
        > Selenium Server, Selenium Grid, 
        > Selenium IDE.

    -- Установка (см. "Установка ПО" в начале)

        1. Selenium
            > в Анаконду (оконный интерфейс)
                >> Anaconda Navigator -> Группа Environments
                    + выберете виртуальную среду или создайте её, скопировав имеющуюся как исходную
                    + добавьте канал (channel) "conda-forge"
                    + (в правой части окна) выполните поиск "selenium"
                    + выберете "selenium" и запустите процесс... applay

        1.1. Selenium Server (Grid)
            https://www.selenium.dev/documentation/grid/
            https://habr.com/ru/post/248559/
            -- Selenium server необходим в случаях, когда вы хотите использовать remote WebDriver [удаленный — Прим. пер.]. За дополнительной информацией обращайтесь к разделу Использование Selenium с remote WebDriver.
            
        2. WebDriver 
            https://www.selenium.dev/documentation/webdriver/
            https://www.selenium.dev/selenium/docs/api/py/index.html
            
            Внимание! Версия драйвера должна соответствовать версии браузера
            > FireFox geckodriver, установка:
                >> Anaconda Navigator 
                    -- найти geckodriver и запустить установку
                    -- поиском найти "geckodriver.exe" и путь до него использовать в ...
                        from selenium.webdriver.firefox.service import Service
                        webdriver_firefox_path = r"C:\Programs\Anaconda3\envs\scrapy\Scripts\geckodriver.exe"
                        webdriver_firefox_service = Service(firefox_gekodriver_path)
                        driver = webdriver.Firefox(service=webdriver_firefox_service)
            > Chrome 
            > Edge
    Сервис
        -- указание маршрута к драйверу

            from selenium.webdriver.chrome.service import Service as ChromeService
            # ...
            service = Service(r"C:\Programs\Anaconda3\envs\scrapy\Scripts\geckodriver.exe")

        -- WebDriver MANAGER for Python
           > Существет несколько менеджеров: 
                >> webdriver-manager 3.8.5 (python 3.7, 3.8, 3.9, 3.10)
                    https://pypi.org/project/webdriver-manager/
                    (входи в Anaconda (можно загрузить при помощи ANACONDA.NAVIGATOR))

                        pip install webdriver-manager

                >> webdrivermanager 0.10.0
                    https://pypi.org/project/webdrivermanager/
                    webdrivermanager 0.10.0
                    (входи в Anaconda (можно загрузить при помощи ANACONDA.NAVIGATOR))

                        pip install webdrivermanager

                >> r-webdriver -- conda-forge
                    https://anaconda.org/conda-forge/r-webdriver
                    (входи в Anaconda (можно загрузить при помощи ANACONDA.NAVIGATOR))
                    A client for the 'WebDriver' 'API'. It allows driving a (probably headless) web browser, and can be used to test web applications, including 'Shiny' apps. In theory it works with any 'WebDriver' implementation, but it was only tested with 'PhantomJS'.

                    Что-то универсальное...
                    
                        conda install -c conda-forge r-webdriver
                        conda install -c "conda-forge/label/cf201901" r-webdriver
                        conda install -c "conda-forge/label/cf202003" r-webdriver
                        conda install -c "conda-forge/label/gcc7" r-webdriver


           > Методы могут по разному называться: 
                install()                # webdriver-manager 3.8.5
                download_and_install()   # webdrivermanager 0.10.0
                    
           > управляющий (менеджер) для веб-драйвера, упрощает работу (если удастся его установить)
           > ВКУСНОЕ: использование ...DriverManager для получения пути к драйверу.

            https://www.selenium.dev/documentation/webdriver/getting_started/install_drivers/
            https://github.com/SergeyPirogov/webdriver_manager#use-with-chrome

            > Инструмент сам получает путь (расположение на host) к драйверу

                service = ChromeService(ChromeDriverManager().install())  # webdriver-manager 3.8.5
                service = FirefoxService(GeckoDriverManager().install())  # webdriver-manager 3.8.5

            > Использование -- selenium 4 (см. ссылки выше, для полной инормации)
            
                from selenium import webdriver
                from selenium.webdriver.chrome.service import Service as ChromeService
                from webdriver_manager.chrome import ChromeDriverManager
                    или 
                from webdrivermanager.chrome import ChromeDriverManager

                driver = webdriver.Chrome(service=ChromeService(ChromeDriverManager().install()))
                НЕ РАБОТАЕТ!!!
                webdriver_chrome_service = chromeService(ChromeDriverManager().download_and_install('latest'))
                # webdriver_chrome_service = chromeService(ChromeDriverManager().download())


    Подключение к драйверу (driver) и параметры (options)
        (к службе (или API или web-драйверу) управления браузером)
        https://www.selenium.dev/documentation/webdriver/drivers/options/

        a) Рабочее (новое)

            1. FireFox
                from selenium.webdriver.firefox.service import Service  # подключение к драйверу
                from selenium.webdriver.firefox.options import Options  # опции запуска

                options = Options()  # экземпляр класса опций
                options.set_preference('permissions.default.image', 2)  # отключение картинок
                options.set_preference("javascript.enabled", True)
                options.set_preference('permissions.default.stylesheet', 1)  # Disable (2) or Enable (1) CSS

                # Сервис
                service = ChromeService(r"C:\Programs\Anaconda3\envs\scrapy\Scripts\geckodriver.exe")

                # Опции (аргументы)
                options = Options()
                options.add_argument('--headless')  # или options.headless = False  # откл. интерфейса: или True, если нужно включить невидимость

                driver = webdriver.Firefox(options=options, service=webdriver_firefox_service)

            2. Chrome
                from selenium.webdriver.chrome.service import Service  # as СhromeService
                from selenium.webdriver.chrome.options import Options  # as СhromeOptions

                options = Options()  # экземпляр класса опций
                options.headless = False  # или True, если нужно включить невидимость  # или options.add_argument('--headless')
                driver = webdriver.Chrome(options=options, service=service)

        б) Переходное с поддержкой сторого в новых подходах  
            options.profile = profile
            webdriver_firefox_service = Service(r"C:\Programs\Anaconda3\pkgs\geckodriver-0.32.0-h611cf2b_0\Scripts\geckodriver.exe")
            driver = webdriver.Firefox(options=options, service=webdriver_firefox_service)

        в) Устаревающее (в будующем не поддерживается)
            from selenium import webdriver
            from selenium.webdriver.firefox.firefox_profile import FirefoxProfile

            profile = FirefoxProfile()
            profile.set_preference('permissions.default.image', 2)  # отключение картинок

            driver = webdriver.Firefox(profile, executable_path='./geckodriver')  # Linux
            driver = webdriver.Firefox(profile, executable_path=r"C:\Programs\Anaconda3\pkgs\geckodriver-0.32.0-h611cf2b_0\Scripts\geckodriver.exe")

        Удалённый вебдрайвер (Remote WebDriver)

            from selenium import webdriver
            from selenium.webdriver.firefox.options import Options  # опции запуска

            firefox_options = webdriver.Options()
            driver = webdriver.Remote(
                command_executor='http://www.example.com',
                options=firefox_options
            )
            driver.get("http://www.google.com")
            driver.quit() 

        Стратегии загрузки страницы (Browser Options)
            https://www.selenium.dev/documentation/webdriver/drivers/options/

            normal (полностью)
                Ready State: complete -- ждём, пока всё не загрузиться...

                from selenium.webdriver.chrome.options import Options
                options = Options()
                options.page_load_strategy = 'normal'
                driver = webdriver.Chrome(options=options)

            eager (частично)
                Ready State: interactive -- DOM загружен полностью, но другие ресурсы, такие как Картинки, могут ещё подгружаться!

                from selenium.webdriver.chrome.options import Options
                options = Options()
                options.page_load_strategy = 'eager'
                driver = webdriver.Chrome(options=options)

            none (всё равно...)
                Ready State: Any -- не блокирует WebDriver по всему возможному...

                from selenium.webdriver.chrome.options import Options
                options = Options()
                options.page_load_strategy = 'none'
                driver = webdriver.Chrome(options=options)

        ОЖИДАНИЕ: неявное и явное...
            https://www.selenium.dev/documentation/webdriver/waits/
            Expected_conditions:
            https://www.selenium.dev/selenium/docs/api/py/webdriver_support/selenium.webdriver.support.expected_conditions.html

            Неявное (Implicit) ожидание 
                -- указание WebDriver опрашивать DOM определённое количество времени, 
                   если требуемый элемент динамический и недоступен немедленно:
                -- действует на все запросы driver.find_element(...)

                    driver.implicitly_wait(10)

                    driver.get("http://somedomain/url_that_delays_loading")
                    my_dynamic_element = driver.find_element(By.ID, "myDynamicElement")

            Явное ожидание 
                selenium.webdriver.support.expected_conditions  --  класс для ожидания в Python

                -- указание WebDriver ожидать возникновение определённого условия до начала действий.
                    # класс для ожидания в Python:
                    from selenium.webdriver.support import expected_conditions as EC  # условие исключения, класс

                    wait = WebDriverWait(driver, timeout=10)
                    username = wait.until(EC.presence_of_element_located((By.ID, 'username')))
                    username.send_keys('admin')

                    Примечание:
                     1. Объект wait с параметрами явного ожидания можно записать 
                        как атрибут объекта driver:
                            driver.wait = WebDriverWait(
                                driver, 
                                timeout=60,
                                poll_frequency=1,
                                ignored_exceptions=[ElementNotVisibleException, ElementNotSelectableException]
                            )
                     2. Тогда после создания драйвера (со всеми параметрами) его можно использовать 
                        универсально в дальнейшем:
                            driver.wait.until(EC.presence_of_element_located((By.ID, 'username')))

                -- если элемент не появиться в указанное время (timeout), то вызвать ошибку (исключение)

                -- НАСТРОЙКА 
                    > время ожидания (timeout)
                    > частота опроса состояния (poll_frequency)
                    > игнорированиен некоторых типов исключений (NoSuchElementException)
                        https://www.selenium.dev/selenium/docs/api/py/common/selenium.common.exceptions.html

                    from selenium.common.exceptions import ElementNotVisibleException, ElementNotSelectableException

                    driver = Firefox()
                    driver.get("http://somedomain/url_that_delays_loading")

                   Вариант-1, местный
                    wait = WebDriverWait(driver, timeout=10, poll_frequency=1, ignored_exceptions=[ElementNotVisibleException, ElementNotSelectableException])
                    element = wait.until(EC.element_to_be_clickable((By.XPATH, "//div")))

                   Вариант-2, неявный как бы глобальный (задаём в самом WebDriver)
                    driver.wait = WebDriverWait(driver, timeout=10, poll_frequency=1, ignored_exceptions=[ElementNotVisibleException, ElementNotSelectableException])

                   ... далее при любом явном обращении через driver.wait.until(...)
                    el = driver.wait.until(EC.element_to_be_clickable((By.XPATH, "//div")))
                    print(el.text)


                -- EC, expected_conditions, виды ожиданий (until):
                    https://www.selenium.dev/selenium/docs/api/py/webdriver_support/selenium.webdriver.support.expected_conditions.html#selenium.webdriver.support.expected_conditions.visibility_of

                    > суммарно:
                        all_of(*expected_conditions)	
                            Ожидание, что все перечисленные ожидаемые условия верны
                        any_of(*expected_conditions)
                            Ожидание, что любое из нескольких ожидаемых условий истинно.
                    > оповещение присутствует (alert is present)
                        alert_is_present()
                    > элемент существует (локатор)
                        presence_of_element_located
                            Ожидание проверки наличия элемента в модели DOM страницы.
                        presence_of_all_elements_located (locator)
                            Ожидание проверки того, что на веб-странице присутствует хотя бы один элемент.
                            An expectation for checking that there is at least one element present on a web page.
                    > элемент виден
                        visibility_of (element)
                    > заголовок содержит
                        title_contains (title)
                    > титул... 
                        title_is (title)
                    > несвежесть элемента (element staleness)
                        staleness_of (element)
                    > видимый текст (visible text)
                        visibility_of(element)
                        visibility_of_all_elements_located (locator)
                        visibility_of_any_elements_located (locator)
                        visibility_of_element_located (locator)
                    > url ...
                        url_changes(url)
                        url_contains (url)

    Действия в браузере
        Переход на страницу по ссылке:
            driver.get(url)
        Ожидаем (с таймаутом 30 сек) загрузки страницы по какому-то элементу:
            from selenium.webdriver.support.ui import WebDriverWait
                WebDriverWait(driver, 30).until(EC.presence_of_element_located((By.CLASS_NAME, 'quotes')))
        Скрипт: запускаем команду в терминале браузера
            1. Получаем (возвращаем, return) высоту страницы:
                body_height = driver.execute_script('return document.body.scrollHeight')
            2. Прокручиваем страницу
                driver.execute_script(f"window.scrollTo(0, {body_height})")
        Клик
            > нажатие на кнопку на экране мышкой
                driver.click()
        Action
            from selenium.webdriver.common.keys import Keys
            from selenium.webdriver.common.action_chains import ActionChains

            actions = ActionChains(driver)  # подключение к классу Действий по открытому драйверу

            -- Нажимаем клавишу "пробел":
                actions.send_keys(Keys.SPACE).perform()  

    Получение данных с сайта 
        Весь HTML
            # html
                html = driver.page_source

        Текст тега (элемента) доступен по атрибуту "text" (а не метод .text() или get())
            .text
                driver.driver.find_element(By.XPATH, '//div[@class="container"]/div[@class="row"]/div[contains(@class, "col-md-offset-4")]').text

    Загрузка данных на сайт
        from selenium.webdriver.remote.file_detector import LocalFileDetector
        driver.file_detector = LocalFileDetector()

        driver.get("http://sso.dev.saucelabs.com/test/guinea-file-upload")
        driver.find_element(By.ID, "myfile").send_keys("/Users/sso/the/local/path/to/darkbulb.jpg")


    Сценарии (FireFox, Chrome)
        Авторизация 
            > сайт начинается со страницы авторизации
              необходимо выполнить авторизацию для получения доступа к страницам сайта

            1-f Инициализация службы управления браузером "Service"
            
                from selenium.webdriver.firefox.service import Service as Service
                service = Service(r"C:\Programs\Anaconda3\envs\scrapy\Scripts\geckodriver.exe")

            2-f Подключение через работающую службу (API-управления) к браузеру

                from selenium import webdriver
                driver = webdriver.Firefox(service=service)

            1-c Хром
                from selenium.webdriver.chrome.service import Service as chromeService
                service = Service(r"C:\Programs\Anaconda3\envs\scrapy\Lib\site-packages\chromedriver_binary\chromedriver.exe")
             или
                service = Service(r"C:\Programs\WebDriver\chromedriver.exe")

                webdriver_chrome_service = chromeService(webdriver_chrome_path)
                driver = webdriver.Chrome(service=webdriver_chrome_service)

            3. Целевой сайт: переход по ссылке с ожиданием загрузки страницы
                > ожидаем страницу по появлению объекта
                > выставляем максимальное время ожидания, переход за границы которого вызовет ошибку

                from selenium.webdriver.common.by import By
                from selenium.webdriver.support.ui import WebDriverWait
                from selenium.webdriver.support import expected_conditions as EC

                driver.get('https://quotes.toscrape.com/login')
                # ждём загрузки контента, определяя это наличию элемента с id='username', таймаут 30 секунд (или возвращаем ошибку)
                WebDriverWait(driver, 30).until(EC.presence_of_element_located((By.ID, 'username')))

            4. Работаем на странице авторизации
                # Имя пользователя: Переход в поле ввода имени пользователя и ввод имени
                login_name = driver.find_element(By.ID, 'username')
                login_name.send_keys('admin')

                # Пароль, вводим: 
                login_password = driver.find_element(By.ID, 'password').send_keys('admin')

                # Кнопка отправки формы на сервер: нажимаем
                login_button = driver.find_element(By.XPATH, '//input[@value="Login"]').click()

            4.1 Результат авторизации: загруженна после авторизации страница

                # html
                html = driver.page_source

                # цитаты, проверка количества

                quotes = driver.find_elements(By.CLASS_NAME, 'quote')           # ищем много элементов -- список 
                # или  = driver.find_elements(By.XPATH, '//*[@class="quote"]')  # ищем список элементов
                # или  = driver.find_element(By.ID, 'password')                 # ищём 1-ин элемент

                print(f"Количество цитат найдено: {len(quotes)=}")
                    -> 10

            5. Закрываем копию браузера
                driver.quit()  # закрываем браузер!

            5.1 Можно так же взять код в менеджер контекста и тогда зарытие автоматически...
                with webdriver.Firefox(service=service) as driver:
                    pass 


        Scroll бесконечный...
            Для бесконечного пролистывания сайта необходимо послать команду пролистывания или нажать пробел... Далее мы можем определить размер блока страницы "body" и, если он изменяется, то процесс дозагрузки идёт, а если нет -- сайт загружен. 
            
            Примечание: для клавиши "пробел" может потребоваться несколько нажатий, чтобы прокрутка дошла до конца имеющейся страницы и вызвала требуемую нам дозагрузку.


        Методы и классы:
            .find_element(By.XPATH | By.CLASS_NAME | By.ID | By.TAG_NAME, "строка")
            .find_elements(...)
                Пример:
                    msg = driver.find_element(By.XPATH, '//div[@class="container"]/div[@class="row"]/div[contains(@class, "col-md-offset-4")]').text
                    print(f"Сообщение сайта: {msg}")
                Внимание:
                    Доступ к значению (тексту) тега выполняется с использованием АТРИБУТА .text,
                        msg = driver.find_element(By.XPATH, '//...').text
                    но не метода или функции msg.text() или msg.get()

            Service -- класс, который подключается к API браузера
            .Firefox(service=service) -- подключение к браузеру через сервис
            .get(url) -- получение страницы в браузере 
            .send_keys('admin')
            .click() -- нажимаем кнопку 
            .execute_script()
                -- запустить на странице скрипт
                    Пример:
                        driver.execute_script(f"windows.scrollTo(0, {last_height});")







ОТЛАДЧИК (Ctrl + Shift + I) -- Анализ страницы 
    -- Вкладки:
        > СЕТЬ-вкладка (Network): позволяет посмотреть, какие запросы были отправлены сайту
            >> чтобы увидеть процесс, нужно обновить страницу
            >> фильтруем запросы по типу (API, ...)
        > HTML-вкладка
        >> может быть отправлен только 1-ин GET-запрос
        > Куки-вкладка: 
            >> куки...
        > Запрос
            >> параметры, которые были отправлены сайту (если были...)
            >> здесь будет, скорее всего, пусто!
        > Ответ-вкладка:
            >> непосредственно сам сайт 
        > Инспектор
            >> вся разметка сайта 


