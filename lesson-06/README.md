`GB` BigData / [Олег Гладкий](https://gb.ru/users/3837199) // домашнее задание

`262698` __Методы сбора и обработки данных из сети Интернет__:  `06`. Scrapy: item.py, pipeslines.py и ItemLoader()

# Scrapy: item.py, pipeslines.py 
System: Windows 7 x64  
Python: 3.8.10  
Scrapy: 2.8.0 (разработка на 2.7.1)  
Splash: 0.9.0 (разработка на 0.7.2)  

## ТЕСТИРОВАНИЕ 


Проект: __splash\_quotes__
   - учебный сайт
   - формирование содержимого при помощи JavaScript 

   Сайт:   http://quotes.toscrape.com/js/

    Пауки:

      author.py -- шаблон основной, статический сайт, заходим на страницу каждого автора + MongoDB

         scrapy crawl author

         -- парсим краткий список учебного сайта "quotes.toscrape.com" для получения ссылок страниц авторов
         -- ныряем в станички автора и парсим их, без повторений
         -- проходим по всем страницам кратного списка (50 стр.)
         -- пишем полученные данных в базу MongoDB
            > имя коллекции определяется по атрибуту collection_name 
            из класса данных Author_SplashQuotesItem(scrapy.Item)
         Прим.: 
            данамический аналог этого сайта не используется, так как не содержит ссылок на авторов...


      quotes.py -- шаблон основной, динамический сайт, Scrapy + SPLASH + MongoDB

         scrapy crawl quotes

         -- парсим краткий список учебного динамического сайта "quotes.toscrape.com/js"
         -- проваливаемся по полученным ссылкам (на индивидуальные страницы) 
            и получаем индивидуальные страницы используя метод SplashRequest(...)
         -- заносим парснутые данные с каждой страницы в базу данны MongoDB
            > имя коллекции определяется по атрибуту collection_name 
            из класса данных Quote_SplashQuotesItem(scrapy.Item)


      quotes_author.py -- шаблон основной

         scrapy crawl pages quotes_author

         -- объединение обоих пауков в один
         -- сайт парсим не динамический, так как последний не содержит ссылок на страницы авторов...

### Тестирование: books_ItemLoader  

Учебный сайт http://books.toscrape.com/

Тестируем:
- запуск из модуля различными способами (reactor, process, ...)
- тестирование технологии ItemLoader c обработкой процессорами в `item.py`

#### паук: books.py 
- шаблон основной, 
- статический сайт, 
- заходим на страницу каждого автора + __MongoDB__
    
Командная строка запуска

    scrapy crawl books

Описание

     - парсим используя ответ 
        > response -- заголовок всей страницы (или уникальные элементы для этой страницы)
        > selector -- группа однотипных элементов (книг), которые в цикле парсим
     - используем разные виды runner-ов, проверка этой технологии

#### паук: books_err.py
Ошибки использования технологии ItemLoader:
1. Использование совместно с классической технологией -- не работает!
2. С объявлением класса -- не задействуются процессоры


### проект: books_kwargs
    Проект: books_kwargs
    Пауки:  books.py
    Запуск: несколько вариантов
    
    - тестирование для аргументов паука
    - передача аргументов от runner или командной строкой (shell-команда)



## ПРОЕКТЫ РАБОЧИЕ: домашнее задание

### проект: __job__

   - Сайты работ
   - Вариант исполнения домашнего задания №3 при помощи Scrapy

#### hh_list.py (job.spiders.hh)
https://hh.ru/search/vacancy/
    
Команда запуск в консоли:

    scrapy crawl hh_list

Описание

- Краткий список вакансий, не заходим на индивидуальную страничку вакансии
- Дубликаты фильтруются базой данных при помощи уникальности ключа
- Pipelines.ru, базы: 

    * MongoDB: job/vacancies_list
      - имя базы задаётся как атрибут класса в pipelines.py
      - имя коллекции задаётся как обычный атрибут класса типа item (уникальный для паука)
      
    * SQLite3: job_base__hh_pages.db/vacancies
      - Имя базы определяется по имени паука классом обработки в piplines.py
      - имя таблицы определяется атрибутом класса pipelines.py -- одинаковое для всех пауков

#### hh_pages.py (job.spiders.hh)
- Splash может быть включён (`splash_mode = False | True`)
- https://hh.ru/search/vacancy/

Команда запуска в консоли:

    scrapy crawl hh_pages

- Проходя по краткому списку заходим на индивидуальные странички вакансий
- Дубликаты фильтруются Scrapy (как-то, я включил это в настройках)
- Возможность использовать Splash: включается атрибутом класса `splash_mode = True|False`
  * сайт статический, так что splash не нужен, но можно включить.
      
- Pipelines.ru, базы данных: 

    * MongoDB: job/vacancies_list
      - имя базы задаётся как атрибут класса в `pipelines.py`
      - имя коллекции задаётся как обычный атрибут класса типа `item` (уникальный для паука)
          
    * SQLite3: job_base__hh_pages.db/vacancies
      - Имя базы определяется по имени паука классом обработки в `piplines.py`
      - имя таблицы определяется атрибутом класса `pipelines.py` -- одинаковое для всех пауков


2022


### проект: __job\_itemloader__

    Сайт: hh.ru
    Паук: hh_list_itemloader.py
    Идея: ItemLoader и процессоры в items.py


__ВОПРОС__

Обрабатывать процессорами в `items.py` очень удобно. Но! В случае со строкой зарплаты возникла заминка: в этой строке присутствуют значения сразу для 3-х полей... Далее, обрабатывая эту сроку процессором поля в items.py мы получим так же три значения (в словаре). Как же эти три значения из одного процессора в поле `salary_min` распределить по трём полям `salary_min`, `salary_max`, `salary_cur`?




2023-03