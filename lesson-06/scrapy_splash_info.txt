GB BigData / [Олег Гладкий | Oleg Gladkiy](https://gb.ru/users/3837199) // домашнее задание
Lesson-06. Scrapy: piplines + splash.


Проект: splash_quotes
   -- учебный сайт
   -- формирование содержимого при помощи JavaScript 

   Сайт:   http://quotes.toscrape.com/js/

   Пауки:

      author.py -- шаблон основной, статический сайт, заходим на страницу каждого автора + MongoDB

         scrapy crawl author

         -- парсим краткий список учебного сайта "quotes.toscrape.com" для получения ссылок страниц авторов
         -- ныряем в станички автора и парсим их, без повторений
         -- проходим по всем страницам кратного списка (50 стр.)
         -- пишем полученные данных в базу MongoDB
            > имя коллекции определяется по атрибуту collection_name 
            из класса данных Author_SplashQuotesItem(scrapy.Item)
         Прим.: 
            данамический аналог этого сайта не используется, так как не содержит ссылок на авторов...


      quotes.py -- шаблон основной, динамический сайт, Scrapy + SPLASH + MongoDB

         scrapy crawl quotes

         -- парсим краткий список учебного динамического сайта "quotes.toscrape.com/js"
         -- проваливаемся по полученным ссылкам (на индивидуальные страницы) 
            и получаем индивидуальные страницы используя метод SplashRequest(...)
         -- заносим парснутые данные с каждой страницы в базу данны MongoDB
            > имя коллекции определяется по атрибуту collection_name 
            из класса данных Quote_SplashQuotesItem(scrapy.Item)


      quotes_author.py -- шаблон основной

         scrapy crawl pages quotes_author

         -- объединение обоих пауков в один
         -- сайт парсим не динамический, так как последний не содержит ссылок на страницы авторов...

Проект: books_ItemLoader
   -- учебный сайт
   -- тестирование технологии ItemLoader

   Сайт:   http://books.toscrape.com/

   Пауки:

      books.py -- шаблон основной, статический сайт, заходим на страницу каждого автора + MongoDB

         scrapy crawl books

         -- парсим используя ответ 
            > response -- заголовок всей страницы (или уникальные элементы для этой страницы)
            > selector -- группа однотипных элементов (книг), которые в цикле парсим
         -- используем разные виды runner-ов, проверка этой технологии

     books_err.py

	-- паука, в котором приводим ошибки применения технологии ItemLoader

Проект: books_kwargs
  -- тестирование для аргументов паука


ПРОЕКТ: job
   -- Сайты работ
   -- Вариант исполнения домашнего задания №3 при помощи Scrapy

   Пауки:

      hh_list.py (job.spiders.hh), паук
         scrapy crawl hh_list
         https://hh.ru/search/vacancy/

         -- Краткий список вакансий, не заходим на индивидуальную страничку вакансии
         -- Дубликаты не фильтруются
         -- Pipelines.ru, базы: 
            MongoDB: job/vacancies_list
               > имя базы задаётся как атрибут класса в pipelines.py
               > имя коллекции задаётся как обычный атрибут класса типа item (уникальный для паука)
            SQLite3: job_base__hh_pages.db/vacancies
               > Имя базы определяется по имени паука классом обработки в piplines.py
               > имя таблицы определяется атрибутом класса pipelines.py -- одинаковое для всех пауков

      hh_pages.py (job.spiders.hh), паук, Splash включён
         scrapy crawl hh_pages
         https://hh.ru/search/vacancy/

         -- Проходя по краткому списку заходим на индивидуальные странички вакансий
         -- Дубликаты фильтруются Scrapy (как-то, я включил это в настройках)
         -- Возможность использовать Splash: включается атрибутом класса "splash_mode = True|False"
            > сайт статический, так что splash не нужен, но можно включить.
         -- Pipelines.ru, базы: 
            MongoDB: job/vacancies_list
               > имя базы задаётся как атрибут класса в pipelines.py
               > имя коллекции задаётся как обычный атрибут класса типа item (уникальный для паука)
            SQLite3: job_base__hh_pages.db/vacancies
               > Имя базы определяется по имени паука классом обработки в piplines.py
               > имя таблицы определяется атрибутом класса pipelines.py -- одинаковое для всех пауков




ПЛАН
   -- MySQL, MongoDB
      > Анализ полученных данных по зарплате: найти по величине из базы.
   -- Синтаксический анализ, преобразовать:
      > зарплату раскидать по соотв. полям используя ItemLoader
   
   -- Аналитика: Демонстрация-запрос к базе (в Юпитер-блокноте) для вывода вакансий с нужной зарплатой.
      > Можно использовать различные БД (запрос к MongoDB, SQLite, MySQL)

   -- SuperJob.ru
   -- Получение данных по API для hh.ru

   Динамические сайты (нужен Splash)

   -- labirint.ru и/или book24.ru
      > здесь для перехода на следующую страницу списка
        >> нужно использовать SPLASH для получения ссылки перехода, которая формируется java-script
        >> или расчитать это количество и формировать переход самостоятельно, добавлением параметра page_nnn
      > анализ форм и ответа на них, чтобы парсить вложенные страницы (а не краткий список).

